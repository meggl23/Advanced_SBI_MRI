{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d89c2a-36e7-48c2-85cf-767c76b127eb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e59e5-1132-4515-974e-9420a6faa392",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3956fd6e-444b-4c00-bab8-5f927e810c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import scipy as sp\n",
    "from scipy.special import j0, jv\n",
    "from scipy.optimize import bisect\n",
    "import scipy.stats as stats\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "from math import *\n",
    "np.seterr(over='ignore')\n",
    "\n",
    "from DTIFuncs import *\n",
    "\n",
    "import dill as pickle\n",
    "\n",
    "from dipy.data import get_fnames\n",
    "from dipy.io.gradients import read_bvals_bvecs\n",
    "from dipy.core.sphere import disperse_charges, Sphere, HemiSphere\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.mplot3d import Axes3D  # for 3D plotting\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from sbi.inference import SNPE\n",
    "from sbi import analysis as analysis\n",
    "\n",
    "\n",
    "Bessel = False\n",
    "\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7143f0a-40e2-4bd2-ae19-cf305833317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['legend.frameon'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ae50ec-b88c-450a-83c1-04f05df1881b",
   "metadata": {},
   "source": [
    "## Bessel params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b4727-11b0-4a0f-b86d-931097d9d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def j1_derivative(x):\n",
    "    \"\"\"Derivative of J1(x) using the identity: J1'(x) = 0.5 * (J0(x) - J2(x)).\"\"\"\n",
    "    return 0.5 * (j0(x) - j2(x))\n",
    "\n",
    "def j2(x):\n",
    "    \"\"\"Bessel function J_2(x).\"\"\"\n",
    "    return jv(2, x)\n",
    "\n",
    "def j1prime_zeros(n, x_max=100, step=0.1):\n",
    "    \"\"\"\n",
    "    Find the first n positive roots of J1'(x) by scanning from x=0 to x_max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n     : int\n",
    "        Number of roots to find\n",
    "    x_max : float\n",
    "        Maximum x to search\n",
    "    step  : float\n",
    "        Step size for scanning sign changes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    zeros : list of float\n",
    "        List of the first n roots (x > 0) of J1'(x).\n",
    "    \"\"\"\n",
    "    zeros = []\n",
    "    x_vals = np.arange(0.0, x_max, step)\n",
    "    \n",
    "    f_prev = j1_derivative(x_vals[0])\n",
    "    for i in range(1, len(x_vals)):\n",
    "        f_curr = j1_derivative(x_vals[i])\n",
    "        # Check for a sign change in [x_vals[i-1], x_vals[i]]\n",
    "        if f_prev * f_curr < 0:\n",
    "            root = bisect(j1_derivative, x_vals[i-1], x_vals[i])\n",
    "            zeros.append(root)\n",
    "            if len(zeros) == n:\n",
    "                break\n",
    "        f_prev = f_curr\n",
    "    \n",
    "    return zeros\n",
    "\n",
    "n_roots = 100\n",
    "Bessel_roots = np.array(j1prime_zeros(n_roots, x_max=10e6, step=0.01))\n",
    "Bessel = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d3f04-b2b2-49d5-a310-8dab52455f7d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1abaef-280c-41d2-91ff-faea487abf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombSignal_poisson(bvecs, bvals, Delta, delta, params):\n",
    "    \"\"\"\n",
    "    Compute the combined diffusion signal in a fast, vectorized way.\n",
    "    \n",
    "    Parameters:\n",
    "      bvecs  : (M,3) array of b-vectors.\n",
    "      bvals  : (M,) array of b-values.\n",
    "      Delta, delta : acquisition parameters (scalars)\n",
    "      params : list/tuple of parameters:\n",
    "          params[0] : fiber directions as an (N,2) array of spherical angles (theta, phi)\n",
    "          params[1] : Dpar (scalar)\n",
    "          params[2] : Dperp (scalar)\n",
    "          params[3] : D (for hindered compartment; passed to vals_to_mat)\n",
    "          params[4] : fiber fractions as an (N+1,) array \n",
    "                      (first element for hindered compartment, then one per fiber)\n",
    "          params[5] : mean (scalar, for gamma distribution)\n",
    "          params[6] : sig2 (scalar, for gamma distribution)\n",
    "          params[7] : S0 (scalar)\n",
    "    \n",
    "    Returns:\n",
    "      Signal : (M,) array of simulated signal values.\n",
    "    \"\"\"\n",
    "    # Unpack parameters\n",
    "    V_angles, Dpar, Dperp, D, fracs, mean, S0 = params\n",
    "\n",
    "    # --- 1. Compute fiber unit vectors from spherical angles ---\n",
    "    # Assume V_angles is an (N,2) array: each row is (theta, phi).\n",
    "    theta_fibers = V_angles[:, 0]\n",
    "    phi_fibers   = V_angles[:, 1]\n",
    "    V_unit = np.column_stack((np.sin(theta_fibers) * np.cos(phi_fibers),\n",
    "                              np.sin(theta_fibers) * np.sin(phi_fibers),\n",
    "                              np.cos(theta_fibers)))  # shape: (N, 3)\n",
    "\n",
    "    # --- 2. Compute angles between each fiber and each b-vector ---\n",
    "    # Make sure bvecs is an array.\n",
    "    bvecs = np.asarray(bvecs)  # shape: (M,3)\n",
    "    M = bvecs.shape[0]\n",
    "    N = V_unit.shape[0]\n",
    "\n",
    "    # Precompute norms of bvecs (we assume fibers are unit length so no extra norm is needed)\n",
    "    bvec_norms = np.linalg.norm(bvecs, axis=1)\n",
    "    # Avoid division by zero:\n",
    "    safe_bvec_norms = np.where(bvec_norms == 0, 1, bvec_norms)\n",
    "\n",
    "    # Compute the dot products for each fiber with all bvecs:\n",
    "    # This gives a (N, M) array where the (i,j) element = v_i dot bvec_j.\n",
    "    dots = V_unit @ bvecs.T  # shape: (N, M)\n",
    "\n",
    "    # Divide each column j by the norm of bvec j (broadcasting over fibers)\n",
    "    cos_angles = dots / safe_bvec_norms  # shape: (N, M)\n",
    "    cos_angles = np.clip(cos_angles, -1, 1)\n",
    "    # Get the angles in [0,pi]\n",
    "    Angs = np.arccos(cos_angles)\n",
    "    # For bvecs that are zero (norm==0), force the angle to zero.\n",
    "    if np.any(bvec_norms == 0):\n",
    "        Angs[:, bvec_norms == 0] = 0\n",
    "    # If an angle is greater than pi/2, use pi - angle.\n",
    "    Angs = np.where(Angs > np.pi/2, np.pi - Angs, Angs)\n",
    "    # In the original code the first measurement was forced to zero (presumably b = 0)\n",
    "    Angs[:, 0] = 0\n",
    "\n",
    "    # --- 3. Precompute the gamma-distributed weights for the integration over R ---\n",
    "    # Gamma distribution parameters:\n",
    "    lam = mean*10000\n",
    "    # Define R values (50 points between 0.0001 and 0.005)\n",
    "    R_vals = np.arange(0.0001, 0.01, 0.0001)  # \n",
    "    transR = (R_vals * 10000).astype(int)\n",
    "\n",
    "    weights = (lam**transR) * np.exp(-lam) / np.array([math.factorial(r) for r in transR.astype(int)]).astype(np.double)\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    # --- 4. Precompute the \"sumterm\" that appears in the restricted compartment ---\n",
    "    # Here we use m=10 terms and assume that a global array Bessel_roots is available.\n",
    "    m = 10\n",
    "    br = Bessel_roots[:m]  # shape: (m,)\n",
    "    br2 = br**2\n",
    "    br6 = br**6\n",
    "    # For each R in R_vals, compute the sumterm.\n",
    "    # We need to broadcast over R and over the m terms.\n",
    "    R2 = R_vals**2  # shape: (50,)\n",
    "    # numerator: shape (50, m)\n",
    "    num = (2 * Dperp * br2 * delta / R2[:, None] - 2 +\n",
    "           2 * np.exp(-Dperp * br2 * delta / R2[:, None]) +\n",
    "           2 * np.exp(-Dperp * br2 * Delta / R2[:, None]) -\n",
    "           np.exp(-Dperp * br2 * (Delta - delta) / R2[:, None]) -\n",
    "           np.exp(-Dperp * br2 * (Delta + delta) / R2[:, None]))\n",
    "    # denominator: shape (50, m)\n",
    "    den = (Dperp**2) * br6 * (br2 - 1) / (R_vals[:, None]**6)\n",
    "    sumterm_R = np.sum(num / den, axis=1)  # shape: (50,)\n",
    "\n",
    "    # --- 5. Compute the restricted compartment signal ---\n",
    "    # For each fiber orientation i (i = 0...N-1) and for each measurement j (j = 0...M-1)\n",
    "    # we need to compute:\n",
    "    #   Restricted(b, theta, R) = exp(-b * (cos(theta)**2) * Dpar) *\n",
    "    #                             exp(-2 * b * (sin(theta)**2) / ((Delta-delta/3)*delta**2) * sumterm)\n",
    "    #\n",
    "    # Notice that only the second exponential depends on R (via sumterm_R) and we need to integrate\n",
    "    # over R with weights.\n",
    "    #\n",
    "    # Compute the part independent of R (base) and the factor x that multiplies sumterm_R.\n",
    "    #\n",
    "    # Angs has shape (N, M) (one row per fiber) and bvals is (M,).\n",
    "    # (We assume that bvals is a 1D array; if not, cast it with np.asarray(bvals).)\n",
    "    bvals = np.asarray(bvals)  # shape: (M,)\n",
    "    base = np.exp(-bvals * (np.cos(Angs)**2) * Dpar)  # shape: (N, M)\n",
    "    # Factor multiplying sumterm_R inside the second exponential.\n",
    "    x = -2 * bvals * (np.sin(Angs)**2) / ((Delta - delta/3) * delta**2)  # shape: (N, M)\n",
    "    # For each fiber orientation and measurement, we want to compute:\n",
    "    #    f(i,j) = sum_{r=0}^{49} weights[r] * exp( x(i,j) * sumterm_R[r] )\n",
    "    # We can compute the 3D array exp(x * sumterm_R) with shape (N, M, 50) and then contract out the last axis.\n",
    "    exp_term = np.exp(x[..., None] * sumterm_R)  # shape: (N, M, 50)\n",
    "    # Now take the weighted sum over the last axis (the R axis):\n",
    "    restricted_integral = np.tensordot(exp_term, weights, axes=([2], [0]))  # shape: (N, M)\n",
    "    # The restricted compartment signal for each fiber and measurement is then:\n",
    "    Res = base * restricted_integral  # shape: (N, M)\n",
    "    #\n",
    "    # Finally, combine the fibers by weighting each fiber's contribution by its fraction.\n",
    "    # The original code did: np.sum([f * R for f,R in zip(fracs[1:],Res)], axis=0)\n",
    "    # That is equivalent to a dot product: (fracs[1:]) dot (each row of Res).\n",
    "    restricted_signal = np.dot(fracs[1:], Res)  # shape: (M,)\n",
    "\n",
    "    # --- 6. Compute the hindered compartment signal ---\n",
    "    # Compute the diffusion tensor from D (using your vals_to_mat function).\n",
    "    dh = vals_to_mat(D)\n",
    "    # The hindered signal is given by:\n",
    "    #    Hi = exp(-b * s)\n",
    "    # where s = sum((bvec @ dh)*bvec, axis=1). Here bvecs is (M,3).\n",
    "    s = np.sum((bvecs @ dh) * bvecs, axis=1)  # shape: (M,)\n",
    "    hindered_signal = np.exp(-bvals * s)  # shape: (M,)\n",
    "\n",
    "    # --- 7. Combine compartments and scale by S0 ---\n",
    "    Signal = fracs[0] * hindered_signal + restricted_signal\n",
    "    return S0 * Signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431c9a9-e3f0-4ab8-9904-3cd8ee54afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenRicciNoise(signal,S0,snr):\n",
    "\n",
    "    size = signal.shape\n",
    "    sigma = S0 / snr\n",
    "    noise1 = np.random.normal(0, sigma, size=size)\n",
    "    noise2 = np.random.normal(0, sigma, size=size)\n",
    "\n",
    "    return np.sqrt((signal+noise1) ** 2 + noise2 ** 2)\n",
    "\n",
    "\n",
    "def AddNoise(signal,S0,snr):\n",
    "    \n",
    "    return GenRicciNoise(signal,S0,snr)\n",
    "\n",
    "def SpherAng(v_in):\n",
    "\n",
    "    if v_in[2] < 0:\n",
    "        v_in = -v_in  # Flip the vector to the top hemisphere\n",
    "\n",
    "    x, y, z = v_in\n",
    "    r = np.linalg.norm(v_in)\n",
    "    if r == 0:\n",
    "        # Degenerate vector, define angles however you like:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    # Polar angle in [0, pi]\n",
    "    theta = np.arccos(z / r)\n",
    "    \n",
    "    # Azimuthal angle in (-pi, pi]\n",
    "    phi = np.arctan2(y, x)\n",
    "        \n",
    "    return theta,phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4c78c-3331-4b0a-b75d-9d8ffdb3243a",
   "metadata": {},
   "source": [
    "## Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32fdca-3813-4899-8330-7fefb05f1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simulator_new(params,bvecs,bvals,Delta,S0=1):\n",
    "    new_params = [np.array([params[:2]]),params[2],params[3],params[4:10],[params[10],1-params[10]],params[11],S0]\n",
    "    Sig = []\n",
    "    for bve,bva,d in zip(bvecs,bvals,Delta):\n",
    "        Sig.append(CombSignal_poisson(bve,bva,d,delta,new_params))\n",
    "    return np.hstack(Sig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050016ca-ed74-47aa-aee3-a01ba5ca5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(params,TrueSig,bvecs,bvals,Delta):\n",
    "    Signal = Simulator_new(params,bvecs,bvals,Delta,S0=1)\n",
    "    return TrueSig - Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f0a1d-7546-41e0-ba5b-38b23e8ca090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals_S0(params,TrueSig,bvecs,bvals,Delta):\n",
    "    Signal = Simulator_new(params,bvecs,bvals,Delta,S0=params[-1])\n",
    "    return TrueSig - Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb37bd2-0096-4fb8-8051-10569e54ed50",
   "metadata": {},
   "source": [
    "## Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68acb18-028d-4207-879c-db45ea021a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Errors(TrueSig,TrueParams,GuessParams,Delta,bvecs,bvals):\n",
    "\n",
    "    Res = np.linalg.norm(residuals(GuessParams,TrueSig,bvecs,bvals,Delta))\n",
    "    alpha_err = np.abs(GuessParams[11]-TrueParams[11])\n",
    "\n",
    "    angle_err1 =  np.abs(GuessParams[0]-TrueParams[0])\n",
    "    angle_err2 =  np.abs(GuessParams[1]-TrueParams[1])\n",
    "\n",
    "    Dpar_err  = np.abs(TrueParams[2]-GuessParams[2])\n",
    "    Dperp_err  = np.abs(TrueParams[3]-GuessParams[3])\n",
    "\n",
    "    MD_guess = np.linalg.eigh(vals_to_mat(GuessParams[4:10]))[0].mean()\n",
    "    MD_true = np.linalg.eigh(vals_to_mat(TrueParams[4:10]))[0].mean()\n",
    "\n",
    "    FA_guess = FracAni(np.linalg.eigh(vals_to_mat(GuessParams[4:10]))[0],MD_guess)\n",
    "    FA_true  = FracAni(np.linalg.eigh(vals_to_mat(TrueParams[4:10]))[0],MD_true)\n",
    "\n",
    "    MD_err = np.abs(MD_guess-MD_true)\n",
    "    FA_err = np.abs(FA_guess-FA_true)\n",
    "\n",
    "    Frac_err  = np.abs(TrueParams[10]-GuessParams[10])\n",
    "\n",
    "    return Res, alpha_err,angle_err1,angle_err2,Dpar_err,Dperp_err,MD_err,FA_err,Frac_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf1d95-b253-40ae-ae92-05d3e80d1e44",
   "metadata": {},
   "source": [
    "# Basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b6d0c-ee9a-48b9-bbf6-0e785a1d14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta = [0.017, 0.035, 0.061]             # ms\n",
    "delta = 0.007           # ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b72b9-9b98-4d11-9960-97538270ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "n_pts = 90\n",
    "theta = np.pi * np.random.random(n_pts)\n",
    "phi = 2 * np.pi * np.random.random(n_pts)\n",
    "hsph_initial = HemiSphere(theta=theta, phi=phi)\n",
    "hsph_updated, potential = disperse_charges(hsph_initial, 5000)\n",
    "vertices = hsph_updated.vertices\n",
    "values = np.ones(45)\n",
    "bvecs = np.vstack((vertices))\n",
    "bvecs = np.insert(bvecs, 0, np.array([0, 0, 0]), axis=0)\n",
    "bvals = np.hstack((0,2000 * values[:-1],[4000]*46))\n",
    "bvecs = np.vstack([bvecs,bvecs,bvecs])\n",
    "bvals = np.hstack([bvals,bvals,bvals])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79d9d8-c86f-4871-9a0b-cc22dcdf0e4c",
   "metadata": {},
   "source": [
    "# Simulation verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8567b-9fbe-4343-9906-bd4cde866c98",
   "metadata": {},
   "source": [
    "## Full set of acquisitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b6a0f1-aa4b-487c-a53c-e3b56f487f7b",
   "metadata": {},
   "source": [
    "### SBI - training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04190c-1d97-4905-9f91-d37e9ecef9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "NumSamps = 200000\n",
    "\n",
    "# Directions\n",
    "x1  = np.random.randn(NumSamps)\n",
    "y1  = np.random.randn(NumSamps)\n",
    "z1  =  np.random.randn(NumSamps)\n",
    "VS = np.vstack([x1,y1,z1])\n",
    "VS = (VS/np.linalg.norm(VS,axis=0)).T\n",
    "AngsS = np.array([SpherAng(v) for v in VS])\n",
    "\n",
    "#Diffusion of restricted\n",
    "DparS  = np.random.rand(NumSamps)*5e-3\n",
    "DperpS = np.random.rand(NumSamps)*5e-3\n",
    "\n",
    "#Diffusion of hindered\n",
    "Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHindS = np.array([ComputeDTI(p) for p in Params])\n",
    "DHindS = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHindS])\n",
    "\n",
    "meanS = np.random.rand(NumSamps)*0.005+1e-4\n",
    "sig2S = np.random.rand(NumSamps) * (4e-7 - 9e-8) + 9e-8\n",
    "\n",
    "#Fraction of hindered\n",
    "fracS  = np.random.rand(NumSamps)\n",
    "TrainParams = np.column_stack([AngsS,DparS,DperpS,DHindS,fracS,meanS])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711aefd0-654c-4984-befd-9a740bb7bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_path = './Networks/'\n",
    "if not os.path.exists(network_path):\n",
    "    os.makedirs(network_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70c2973-6174-4c0f-bc95-61d349329631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training neural network. Epochs trained: 459"
     ]
    }
   ],
   "source": [
    "if os.path.exists(f\"{network_path}/Full_Sim_50_200k_poisson.pickle\"):\n",
    "    with open(f\"{network_path}/Full_Sim_50_200k_poisson.pickle\", \"rb\") as handle:\n",
    "        posterior = pickle.load(handle)\n",
    "else:\n",
    "    np.random.seed(10)\n",
    "    torch.manual_seed(10)\n",
    "    TrainSigS = []\n",
    "    NoisyTrainSigS = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([AngsS[i]])\n",
    "        dpar = DparS[i]\n",
    "        dperp = DperpS[i]\n",
    "        \n",
    "        dh   = DHindS[i]\n",
    "        f    = [fracS[i],1-fracS[i]]\n",
    "    \n",
    "        a = meanS[i]\n",
    "        s = sig2S[i]\n",
    "        s0 = 1\n",
    "        \n",
    "        Noise = 50\n",
    "        \n",
    "        TrainSig1 = CombSignal_poisson(bvecs[:(n_pts+1)],bvals[:(n_pts+1)],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(bvecs[(n_pts+1):2*(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(bvecs[2*(n_pts+1):],bvals[2*(n_pts+1):],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSigS.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        \n",
    "        NoisyTrainSigS.append(AddNoise(TrainSigS[-1],s0,Noise))\n",
    "    NoisyTrainSigS = np.array(NoisyTrainSigS)\n",
    "\n",
    "    Obs = torch.tensor(NoisyTrainSigS).float()\n",
    "    Par = torch.tensor(TrainParams).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs=100)\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "\n",
    "    with open(f\"{network_path}/Full_Sim_50_200k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14665259-eeaf-45f5-9d8d-05596fed4c8d",
   "metadata": {},
   "source": [
    "### Evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f1b7b0-a14b-43e5-9c06-399f87b24302",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "TestSamps = 20\n",
    "\n",
    "# Directions\n",
    "x1  = np.random.randn(TestSamps)\n",
    "y1  = np.random.randn(TestSamps)\n",
    "z1  =  np.random.randn(TestSamps)\n",
    "V = np.vstack([x1,y1,z1])\n",
    "V = (V/np.linalg.norm(V,axis=0)).T\n",
    "Angs = np.array([SpherAng(v) for v in V])\n",
    "\n",
    "#Diffusion of restricted\n",
    "Dpar  = np.random.rand(TestSamps)*5e-3\n",
    "Dperp = np.random.rand(TestSamps)*5e-3\n",
    "\n",
    "#Diffusion of hindered\n",
    "Params_abc =  np.random.rand(TestSamps,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(TestSamps,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "\n",
    "#Fraction of hindered\n",
    "frac  = np.random.rand(TestSamps)\n",
    "\n",
    "mean = np.random.rand(TestSamps)*0.005+1e-4\n",
    "sig2 = np.random.rand(TestSamps) * (4e-7 - 9e-8) + 9e-8\n",
    "\n",
    "S0Rand =np.ones(TestSamps)\n",
    "\n",
    "TestParams = np.column_stack([Angs,Dpar,Dperp,DHind,frac,mean])\n",
    "\n",
    "TestSig = []\n",
    "NoisyTestSig = []\n",
    "for i in tqdm(range(TestSamps)):\n",
    "    v = np.array([Angs[i]])\n",
    "    dpar = Dpar[i]\n",
    "    dperp = Dperp[i]\n",
    "    \n",
    "    dh   = DHind[i]\n",
    "    f    = [frac[i],1-frac[i]]\n",
    "\n",
    "    a = mean[i]\n",
    "    s = sig2[i]\n",
    "    alpha     = a * a / s\n",
    "    scale = s / a\n",
    "    rv = stats.gamma(a=alpha,scale=scale)\n",
    "    \n",
    "    R = np.linspace(0.0001,0.005, 30)\n",
    "    weights = rv.pdf(R)\n",
    "    weights = weights/np.sum(weights)\n",
    "    fig, ax = plt.subplots(2,1)\n",
    "    ax[0].plot(R,weights)\n",
    "    s0 = 1\n",
    "\n",
    "    TestSig1 = CombSignal_poisson(bvecs[:(n_pts+1)],bvals[:(n_pts+1)],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "    TestSig2 = CombSignal_poisson(bvecs[(n_pts+1):2*(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "    TestSig3 = CombSignal_poisson(bvecs[2*(n_pts+1):],bvals[2*(n_pts+1):],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "    ax[1].plot(TestSig1)\n",
    "    ax[1].plot(TestSig2)\n",
    "    ax[1].plot(TestSig3)\n",
    "    fig.suptitle('a')\n",
    "    plt.show()\n",
    "    TestSig.append(np.hstack([TestSig1,TestSig2,TestSig3]))\n",
    "    Noisy = []\n",
    "    for Noise in [2,10,20,30]:\n",
    "        Noisy.append(AddNoise(TestSig[-1],s0,Noise))\n",
    "    NoisyTestSig.append(Noisy)\n",
    "NoisyTestSig = np.array(NoisyTestSig)\n",
    "NoisyTestSig = np.swapaxes(NoisyTestSig,0,1)\n",
    "TestSig = np.array(TestSig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f537eb41-e5e1-4650-aef0-8fb3f69e3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "Params_abc =  np.random.rand(1,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(1,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind_guess = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind_guess = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind_guess])\n",
    "\n",
    "Dpar_guess = np.random.rand()*1e-3            # mm^2/s\n",
    "Dperp_guess = np.random.rand()*1e-3             # mm^2/s\n",
    "phi = 0#np.random.rand()*pi\n",
    "cos_theta = 0#np.random.rand()  # uniform in [0,1]\n",
    "theta = np.arccos(cos_theta)         # in [0, pi/2]\n",
    "Angs_guess = np.vstack([theta,phi]).T\n",
    "\n",
    "mean_guess = np.random.rand()*0.005 + 1e-4\n",
    "\n",
    "frac_guess = np.random.rand()\n",
    "guess = np.column_stack([Angs_guess,Dpar_guess,Dperp_guess,DHind_guess,frac_guess,mean_guess]).squeeze()\n",
    "bounds = np.array([[-np.inf,np.inf]]*12).T\n",
    "bounds[:,0] = [0,np.pi/2]\n",
    "bounds[:,1] = [-np.pi,np.pi]\n",
    "bounds[:,2] = [0,5e-3]\n",
    "bounds[:,3] = [0,5e-3]\n",
    "bounds[:,4] = [-5e-3,5e-3]\n",
    "bounds[:,5] = [-5e-3,5e-3]\n",
    "bounds[:,6] = [-5e-3,5e-3]\n",
    "bounds[:,7] = [-5e-3,5e-3]\n",
    "bounds[:,8] = [-5e-3,5e-3]\n",
    "bounds[:,9] = [-5e-3,5e-3]\n",
    "bounds[:,10] = [0,1]\n",
    "bounds[:,11] = [1e-4,0.005+1e-4]\n",
    "LS_result = np.zeros([4,20,12])\n",
    "bve_split = [bvecs[:(n_pts+1)],bvecs[(n_pts+1):2*(n_pts+1)],bvecs[2*(n_pts+1):]]\n",
    "bva_split = [bvals[:(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],bvals[2*(n_pts+1):]]\n",
    "for i in tqdm(range(20)):\n",
    "    for j in range(4):\n",
    "        result = sp.optimize.least_squares(residuals, guess, args=[NoisyTestSig[j,i],bve_split,bva_split,Delta],\n",
    "                                      bounds=bounds,verbose=1,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        LS_result[j,i] = result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef6ca8-6490-4e7d-a24a-2f2a9178f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_Errors = []\n",
    "for N in tqdm(LS_result):\n",
    "    temp = []\n",
    "    for n_guess,n_true,sig in zip(N,TestParams,TestSig):\n",
    "        temp.append(Errors(sig,n_true,n_guess,Delta,bve_split,bva_split))\n",
    "    LS_Errors.append(temp)\n",
    "LS_Errors = np.array(LS_Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84190f-d4c6-4aab-83a1-1300be1bee95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the function for optimization\n",
    "def fit_SBI(i,j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posterior.sample((1000,), x=NoisyTestSig[i,j],show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "y_indx = np.repeat(np.arange(20),4)\n",
    "x_indx = np.tile(np.arange(4),20)\n",
    "indices = np.column_stack([x_indx,y_indx])\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(fit_SBI)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "SBI_Res = np.zeros([4,20,13])\n",
    "\n",
    "for i, j, x in results:\n",
    "    SBI_Res[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    SBI_Res[i, j,-2] = np.clip(SBI_Res[i, j,-2],0,100)\n",
    "    \n",
    "SBI_Errors = []\n",
    "for N in tqdm(SBI_Res):\n",
    "    temp = []\n",
    "    for n_guess,n_true,sig in zip(N,TestParams,TestSig):\n",
    "        temp.append(Errors(sig,n_true,n_guess,Delta,bve_split,bva_split))\n",
    "    SBI_Errors.append(temp)\n",
    "SBI_Errors = np.array(SBI_Errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a68fb5b-7c66-4ec1-9e37-447bffef1cc3",
   "metadata": {},
   "source": [
    "## Minimum Acquisitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b3107-6ce8-461f-aa6b-ab6c30dbb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs2000 = bvecs[:91][bvals[:91]==2000]\n",
    "distance_matrix = squareform(pdist(bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs2000_selected = bvecs[:91][bvals[:91]==2000][selected_indices]\n",
    "true_indices = []\n",
    "for b in bvecs2000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs4000 = bvecs[:91][bvals[:91]==4000]\n",
    "distance_matrix = squareform(pdist(bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs4000_selected = bvecs[:91][bvals[:91]==4000][selected_indices]\n",
    "for b in bvecs4000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "MinIdices = np.array(true_indices)\n",
    "DevilIndices = np.hstack([MinIdices,MinIdices+91,MinIdices+182])\n",
    "DevilIndices = np.hstack([0,DevilIndices])\n",
    "bvecs_Dev = bvecs[DevilIndices]\n",
    "bvals_Dev = bvals[DevilIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406596c0-3a10-407e-9f7d-9b711c16b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "NumSamps = 200000\n",
    "\n",
    "# Directions\n",
    "x1  = np.random.randn(NumSamps)\n",
    "y1  = np.random.randn(NumSamps)\n",
    "z1  =  np.random.randn(NumSamps)\n",
    "VS = np.vstack([x1,y1,z1])\n",
    "VS = (VS/np.linalg.norm(VS,axis=0)).T\n",
    "AngsS = np.array([SpherAng(v) for v in VS])\n",
    "\n",
    "#Diffusion of restricted\n",
    "DparS  = np.random.rand(NumSamps)*5e-3\n",
    "DperpS = np.random.rand(NumSamps)*5e-3\n",
    "\n",
    "#Diffusion of hindered\n",
    "Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHindS = np.array([ComputeDTI(p) for p in Params])\n",
    "DHindS = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHindS])\n",
    "\n",
    "meanS = np.random.rand(NumSamps)*0.005+1e-4\n",
    "\n",
    "#Fraction of hindered\n",
    "fracS  = np.random.rand(NumSamps)\n",
    "TrainParams = np.column_stack([AngsS,DparS,DperpS,DHindS,fracS,meanS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a7bcd6-11f8-45de-b441-3a7712406b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/Dev_Sim_50_200k_poisson.pickle\"):\n",
    "    with open(f\"{network_path}/Dev_Sim_50_200k_poisson.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "\n",
    "    np.random.seed(10)\n",
    "    torch.manual_seed(10)\n",
    "    TrainSigS = []\n",
    "    NoisyTrainSigS = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([AngsS[i]])\n",
    "        dpar = DparS[i]\n",
    "        dperp = DperpS[i]\n",
    "        \n",
    "        dh   = DHindS[i]\n",
    "        f    = [fracS[i],1-fracS[i]]\n",
    "    \n",
    "        a = meanS[i]\n",
    "        s0 = 1\n",
    "        \n",
    "        Noise = 50\n",
    "        \n",
    "        TrainSig1 = CombSignal_poisson(bvecs_Dev[:7],bvals_Dev[:7],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(bvecs_Dev[7:13],bvals_Dev[7:13],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(bvecs_Dev[13:],bvals_Dev[13:],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSigS.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        \n",
    "        NoisyTrainSigS.append(AddNoise(TrainSigS[-1],s0,Noise))\n",
    "    NoisyTrainSigS = np.array(NoisyTrainSigS)\n",
    "\n",
    "\n",
    "    Obs = torch.tensor(NoisyTrainSigS).float()\n",
    "    Par = torch.tensor(TrainParams).float()\n",
    "    \n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs=100)\n",
    "    posteriorMin = inference.build_posterior(density_estimator)\n",
    "\n",
    "    with open(f\"{network_path}/Dev_Sim_50_200k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posteriorMin, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0fd0e2-b41f-493c-bbf8-c6b199e9fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for optimization\n",
    "def fit_SBI(i,j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posteriorMin.sample((1000,), x=NoisyTestSig[i,j][DevilIndices],show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "y_indx = np.repeat(np.arange(20),4)\n",
    "x_indx = np.tile(np.arange(4),20)\n",
    "indices = np.column_stack([x_indx,y_indx])\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(fit_SBI)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "SBI_Res = np.zeros([4,20,12])\n",
    "\n",
    "for i, j, x in results:\n",
    "    SBI_Res[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    SBI_Res[i, j,-2] = np.clip(SBI_Res[i, j,-2],0,100)\n",
    "    \n",
    "SBI_Errors_Min = []\n",
    "for N in tqdm(SBI_Res):\n",
    "    temp = []\n",
    "    for n_guess,n_true,sig in zip(N,TestParams,TestSig):\n",
    "        temp.append(Errors(sig,n_true,n_guess,Delta,bve_split,bva_split))\n",
    "    SBI_Errors_Min.append(temp)\n",
    "SBI_Errors_Min = np.array(SBI_Errors_Min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0396e0f4-84d8-4b3b-9439-a4f9ba23f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = np.array([[-np.inf,np.inf]]*12).T\n",
    "bounds[:,0] = [0,np.pi]\n",
    "bounds[:,1] = [0,2*np.pi]\n",
    "bounds[:,2] = [0,5e-3]\n",
    "bounds[:,3] = [0,5e-3]\n",
    "bounds[:,4] = [-5e-3,5e-3]\n",
    "bounds[:,5] = [-5e-3,5e-3]\n",
    "bounds[:,6] = [-5e-3,5e-3]\n",
    "bounds[:,7] = [-5e-3,5e-3]\n",
    "bounds[:,8] = [-5e-3,5e-3]\n",
    "bounds[:,9] = [-5e-3,5e-3]\n",
    "bounds[:,10] = [0,1]\n",
    "bounds[:,11] = [1e-4,0.005+1e-4]\n",
    "LS_result = np.zeros([4,20,12])\n",
    "bve_splitd = [bvecs_Dev[:7],bvecs_Dev[7:13],bvecs_Dev[13:]]\n",
    "bva_splitd = [bvals_Dev[:7],bvals_Dev[7:13],bvals_Dev[13:]]\n",
    "for i in tqdm(range(20)):\n",
    "    for j in range(4):\n",
    "        result = sp.optimize.least_squares(residuals, guess, args=[NoisyTestSig[j,i][DevilIndices],bve_splitd,bva_splitd,Delta],\n",
    "                                      bounds=bounds,verbose=1,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        LS_result[j,i] = result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4148ee0-fd80-4532-83db-9f3bb7660cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_Errors_Min = []\n",
    "for N in tqdm(LS_result):\n",
    "    temp = []\n",
    "    for n_guess,n_true,sig in zip(N,TestParams,TestSig):\n",
    "        temp.append(Errors(sig,n_true,n_guess,Delta,bve_split,bva_split))\n",
    "    LS_Errors_Min.append(temp)\n",
    "LS_Errors_Min = np.array(LS_Errors_Min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b919856-97f5-423f-b9b9-674a83056e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "posterior_samples_1 = posterior.sample((10000,), x=NoisyTestSig[-1][i],show_progress_bars=False)\n",
    "fit_params_full = np.array(posterior_samples_1).mean(axis=0)\n",
    "\n",
    "GuessSig_full = Simulator_new(fit_params_full,bve_split,bva_split,Delta)\n",
    "\n",
    "posterior_samples_1 = posteriorMin.sample((10000,), x=NoisyTestSig[-1][i][DevilIndices],show_progress_bars=False)\n",
    "fit_params_Min = np.array(posterior_samples_1).mean(axis=0)\n",
    "\n",
    "GuessSig_min = Simulator_new(fit_params_Min,bve_split,bva_split,Delta)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(2,1,figsize=(10,4))\n",
    "xsig = np.hstack([np.arange(273)[:90:3],np.arange(273)[91:181:3],np.arange(273)[182::3]])\n",
    "Sig1 = np.hstack([TestSig[i][:90:3],TestSig[i][91:181:3],TestSig[i][182::3]])\n",
    "gSig1 = np.hstack([GuessSig_full[:90:3],GuessSig_full[91:181:3],GuessSig_full[182::3]])\n",
    "gSig2 = np.hstack([GuessSig_min[:90:3],GuessSig_min[91:181:3],GuessSig_min[182::3]])\n",
    "ax[0].plot(xsig,Sig1,lw=3,c='k',label='True signal')\n",
    "ax[1].plot(xsig,Sig1,lw=3,c='k')\n",
    "ax[0].plot(xsig,gSig1,lw=3,alpha=0.7,c='lightseagreen',label='SBI fit signal')\n",
    "ax[1].plot(xsig,gSig2,lw=3,alpha=0.7,c='lightseagreen')\n",
    "\n",
    "\n",
    "ax[1].fill_betweenx(np.arange(0,1.1,0.1),0*np.ones(11),7*np.ones(11),color='gray',alpha=0.5)\n",
    "ax[1].fill_betweenx(np.arange(0,1.1,0.1),45*np.ones(11),50*np.ones(11),color='gray',alpha=0.5)\n",
    "ax[0].legend(handlelength=1, # fine-tune the legend's position\n",
    "    frameon=False,\n",
    "    fontsize=24,ncols=2,bbox_to_anchor=(0.8,0.2),columnspacing=0.5,loc=1)\n",
    "ax[1].axis('off')\n",
    "ax[0].axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbfa708-d812-4e24-b09a-6f7c4ceccf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "\n",
    "result = sp.optimize.least_squares(residuals, guess, args=[NoisyTestSig[-1,i],bve_split,bva_split,Delta],\n",
    "                              bounds=bounds,verbose=1,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "fit_params_full = result.x\n",
    "\n",
    "GuessSig_full = Simulator_new(fit_params_full,bve_split,bva_split,Delta)\n",
    "\n",
    "result = sp.optimize.least_squares(residuals, guess, args=[NoisyTestSig[-1,i][DevilIndices],bve_splitd,bva_splitd,Delta],\n",
    "                              bounds=bounds,verbose=1,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "fit_params_Min = result.x\n",
    "\n",
    "GuessSig_min = Simulator_new(fit_params_Min,bve_split,bva_split,Delta)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(2,1,figsize=(10,4))\n",
    "xsig = np.hstack([np.arange(273)[:90:3],np.arange(273)[91:181:3],np.arange(273)[182::3]])\n",
    "Sig1 = np.hstack([TestSig[i][:90:3],TestSig[i][91:181:3],TestSig[i][182::3]])\n",
    "gSig1 = np.hstack([GuessSig_full[:90:3],GuessSig_full[91:181:3],GuessSig_full[182::3]])\n",
    "gSig2 = np.hstack([GuessSig_min[:90:3],GuessSig_min[91:181:3],GuessSig_min[182::3]])\n",
    "ax[0].plot(xsig,Sig1,lw=3,c='k',label='True signal')\n",
    "ax[1].plot(xsig,Sig1,lw=3,c='k')\n",
    "ax[0].plot(xsig,gSig1,lw=3,alpha=0.7,c='darkorange',label='NLLS fit signal')\n",
    "ax[1].plot(xsig,gSig2,lw=3,alpha=0.7,c='darkorange')\n",
    "\n",
    "\n",
    "ax[1].fill_betweenx(np.arange(0,1.1,0.1),0*np.ones(11),7*np.ones(11),color='gray',alpha=0.5)\n",
    "ax[1].fill_betweenx(np.arange(0,1.1,0.1),45*np.ones(11),50*np.ones(11),color='gray',alpha=0.5)\n",
    "ax[0].legend(handlelength=1, # fine-tune the legend's position\n",
    "    frameon=False,\n",
    "    fontsize=24,ncols=2,bbox_to_anchor=(0.8,0.2),columnspacing=0.5,loc=1)\n",
    "ax[1].axis('off')\n",
    "ax[0].axis('off')\n",
    "ax[1].set_ylim([0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7370b676-c922-46e1-930a-4d1f6b2a3532",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "r = 1.0  # sphere radius\n",
    "vector = np.array([-0.5, -1, 1])   # arbitrary vector\n",
    "n = vector / np.linalg.norm(vector)  # unit vector in the direction of 'vector'\n",
    "intersection = n * r  # intersection of the vector with the sphere\n",
    "\n",
    "# Circle parameters (geodesic circle on the sphere)\n",
    "circle_angle_deg = 15  # angular radius in degrees\n",
    "alpha1 = [(S[:,2].mean()) for S in SBI_Errors][-1]\n",
    "\n",
    "# -----------------------------\n",
    "# Construct a circle on the sphere\n",
    "# -----------------------------\n",
    "# To draw a circle on the sphere centered at 'intersection',\n",
    "# we use the following idea:\n",
    "# For a given center n (a point on the unit sphere) and an angular radius alpha,\n",
    "# any point on the circle can be written as:\n",
    "#   P(t) = cos(alpha)*n + sin(alpha)*(cos(t)*u + sin(t)*w)\n",
    "# where u and w are any two orthonormal vectors spanning the tangent plane at n.\n",
    "\n",
    "# First, choose u as a vector perpendicular to n.\n",
    "# (If n is parallel to the z-axis, choose a different axis to avoid the zero vector.)\n",
    "if np.allclose(n, [0, 0, 1]):\n",
    "    u = np.array([1, 0, 0])\n",
    "else:\n",
    "    u = np.cross(n, [0, 0, 1])\n",
    "    u = u / np.linalg.norm(u)\n",
    "\n",
    "# Then, w is perpendicular to both n and u.\n",
    "w = np.cross(n, u)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Create the sphere mesh\n",
    "# -----------------------------\n",
    "phi = np.linspace(0, 2 * np.pi, 500)  # azimuthal angle\n",
    "theta = np.linspace(0, np.pi, 500)      # polar angle\n",
    "\n",
    "phi, theta = np.meshgrid(phi, theta)\n",
    "x_sphere = r * np.sin(theta) * np.cos(phi)\n",
    "y_sphere = r * np.sin(theta) * np.sin(phi)\n",
    "z_sphere = r * np.cos(theta)\n",
    "\n",
    "# -----------------------------\n",
    "# Plot everything\n",
    "# -----------------------------\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "# Plot the vector (using quiver)\n",
    "ax.quiver(0, 0, 0, intersection[0], intersection[1], intersection[2],\n",
    "          color='r', linewidth=2, arrow_length_ratio=0.1)\n",
    "\n",
    "# Plot the circle on the sphere\n",
    "# Create points around the circle\n",
    "t_vals = np.linspace(0, 2 * np.pi, 200)\n",
    "circle_points = np.array([\n",
    "    np.cos(alpha1) * n + np.sin(alpha1) * (np.cos(t) * u + np.sin(t) * w)\n",
    "    for t in t_vals\n",
    "])\n",
    "\n",
    "ax.plot(circle_points[:, 0], circle_points[:, 1], circle_points[:, 2], color='paleturquoise', linewidth=2,ls='--')\n",
    "\n",
    "circle_angle_deg = 15  # angular radius in degrees\n",
    "alpha2 = [(S[:,2].mean()) for S in SBI_Errors_Min][-1]\n",
    "\n",
    "# Plot the circle on the sphere\n",
    "# Create points around the circle\n",
    "t_vals = np.linspace(0, 2 * np.pi, 100)\n",
    "circle_points = np.array([\n",
    "    np.cos(alpha2) * n + np.sin(alpha2) * (np.cos(t) * u + np.sin(t) * w)\n",
    "    for t in t_vals\n",
    "])\n",
    "\n",
    "ax.plot(circle_points[:, 0], circle_points[:, 1], circle_points[:, 2], color='lightseagreen', linewidth=2,ls='--')\n",
    "\n",
    "alpha3 = [(S[:,2].mean()) for S in LS_Errors][-1]\n",
    "\n",
    "# Plot the circle on the sphere\n",
    "# Create points around the circle\n",
    "t_vals = np.linspace(0, 2 * np.pi, 100)\n",
    "circle_points = np.array([\n",
    "    np.cos(alpha3) * n + np.sin(alpha3) * (np.cos(t) * u + np.sin(t) * w)\n",
    "    for t in t_vals\n",
    "])\n",
    "\n",
    "ax.plot(circle_points[:, 0], circle_points[:, 1], circle_points[:, 2], color='sandybrown', linewidth=2,ls='--')\n",
    "\n",
    "alpha4 = [(S[:,2].mean()) for S in LS_Errors_Min][-1]\n",
    "\n",
    "# Plot the circle on the sphere\n",
    "# Create points around the circle\n",
    "t_vals = np.linspace(0, 2 * np.pi, 100)\n",
    "circle_points = np.array([\n",
    "    np.cos(alpha4) * n + np.sin(alpha4) * (np.cos(t) * u + np.sin(t) * w)\n",
    "    for t in t_vals\n",
    "])\n",
    "\n",
    "ax.plot(circle_points[:, 0], circle_points[:, 1], circle_points[:, 2], color='darkorange', linewidth=2,ls='--')\n",
    "\n",
    "# Set equal aspect ratio for all axes\n",
    "max_range = r * 1.2\n",
    "for axis in 'xyz':\n",
    "    getattr(ax, 'set_{}lim'.format(axis))((-max_range, max_range))\n",
    "\n",
    "\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot < np.cos(alpha3)) + (dot > np.cos(alpha4))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# -----------------------------\n",
    "# Plot everything\n",
    "# -----------------------------\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot > np.cos(alpha3)) + (dot < np.cos(alpha4))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# Plot the spherical cap (inside the circle) with transparency\n",
    "ax.plot_surface(x_sphere_masked, y_sphere_masked, z_sphere_masked,\n",
    "                color='darkorange',shade=False, alpha=0.5, rstride=2, cstride=2, edgecolor='none')\n",
    "\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot > np.cos(alpha2)) + (dot < np.cos(alpha3))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# Plot the spherical cap (inside the circle) with transparency\n",
    "ax.plot_surface(x_sphere_masked, y_sphere_masked, z_sphere_masked,\n",
    "                color='sandybrown',shade=False, alpha=0.5, rstride=2, cstride=2, edgecolor='none')\n",
    "\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot > np.cos(alpha1)) + (dot < np.cos(alpha2))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# Plot the spherical cap (inside the circle) with transparency\n",
    "ax.plot_surface(x_sphere_masked, y_sphere_masked, z_sphere_masked,\n",
    "                color='lightseagreen',shade=False, alpha=0.5, rstride=2, cstride=2, edgecolor='none')\n",
    "\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot < np.cos(alpha1))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# Plot the spherical cap (inside the circle) with transparency\n",
    "ax.plot_surface(x_sphere_masked, y_sphere_masked, z_sphere_masked,\n",
    "                color='paleturquoise',alpha=0.5,linewidth=0,rstride=1, cstride=1, shade=False,)\n",
    "\n",
    "dot = x_sphere * n[0] + y_sphere * n[1] + z_sphere * n[2]\n",
    "mask = (dot > np.cos(alpha4))\n",
    "# Mask out points that are not in the spherical cap (set them to NaN)\n",
    "x_sphere_masked = np.where(mask, np.nan, x_sphere)\n",
    "y_sphere_masked = np.where(mask, np.nan, y_sphere)\n",
    "z_sphere_masked = np.where(mask, np.nan, z_sphere)\n",
    "\n",
    "# Plot the spherical cap (inside the circle) with transparency\n",
    "ax.plot_surface(x_sphere_masked, y_sphere_masked, z_sphere_masked,\n",
    "                color='gray', alpha=0.2, rstride=2, cstride=2, edgecolor='none')\n",
    "\n",
    "ax.axis('equal')\n",
    "ax.axis('off')\n",
    "ax.view_init(elev=20, azim=-80)\n",
    "\n",
    "minLS_patch = mpatches.Patch(color='darkorange', label='Minimum NLLS')\n",
    "fullLS_patch = mpatches.Patch(color='sandybrown', label='Full NLLS')\n",
    "\n",
    "minSBI_patch = mpatches.Patch(color='lightseagreen', label='Minimum SBI')\n",
    "fullSBI_patch = mpatches.Patch(color='paleturquoise', label='Full SBI')\n",
    "\n",
    "ax.legend(\n",
    "    handles=[minLS_patch,minSBI_patch,fullLS_patch,fullSBI_patch],\n",
    "    loc='lower left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=2,\n",
    "    bbox_to_anchor=(0.18, 0.09),fontsize=18,\n",
    "    columnspacing=0.5,\n",
    "    handlelength=0.8,\n",
    ")\n",
    "ax.set_title('Average angle diff.',x=0.52, y=0.825,fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54996db3-a4ae-4822-8d58-340cce1e56be",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1,alpha2,alpha3,alpha4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a8f65e-b3da-491c-ba6c-a2296c60ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pos = np.array([0, 2, 4,6])*2\n",
    "POSITIONS = g_pos\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors[:,:,1]*1000,)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,4))    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "# Colors\n",
    "BG_WHITE = \"#fbf9f4\"\n",
    "GREY_LIGHT = \"#b4aea9\"\n",
    "GREY50 = \"#7F7F7F\"\n",
    "BLUE_DARK = \"#1B2838\"\n",
    "BLUE = \"#2a475e\"\n",
    "BLACK = \"#282724\"\n",
    "GREY_DARK = \"#747473\"\n",
    "RED_DARK = \"#850e00\"\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='turquoise'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors[:,:,1].T*1000,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors[:,:,1]*1000):\n",
    "    ax.scatter(x, y, s = 100, color='paleturquoise', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+0.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors_Min[:,:,1],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='cadetblue'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors_Min[:,:,1].T*1000,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors_Min[:,:,1]*1000):\n",
    "    ax.scatter(x, y, s = 100, color='darkturquoise', alpha=0.8)\n",
    "\n",
    "\n",
    "POSITIONS = g_pos+1.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors[:,:,1],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='sandybrown'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors[:,:,1].T*1000,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors[:,:,1]*1000):\n",
    "    ax.scatter(x, y, s = 100, color='peachpuff', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+2\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors_Min[:,:,1]*1000,)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "\n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='darkorange'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors_Min[:,:,1].T*1000,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors_Min[:,:,1]*1000):\n",
    "    ax.scatter(x, y, s = 100, color='orange', alpha=0.8)\n",
    "\n",
    "ax.set_xticks([1,5,9,13],['2','10','20','30'],fontsize=24)\n",
    "ax.set_xlabel('SNR',fontsize=32)\n",
    "ax.tick_params(axis='x', labelsize=24)\n",
    "ax.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax.tick_params(axis='y', labelsize=24,)\n",
    "ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "\n",
    "minLS_patch = mpatches.Patch(color='darkorange', label='Minimum NLLS')\n",
    "fullLS_patch = mpatches.Patch(color='sandybrown', label='Full NLLS')\n",
    "\n",
    "minSBI_patch = mpatches.Patch(color='lightseagreen', label='Minimum SBI')\n",
    "fullSBI_patch = mpatches.Patch(color='paleturquoise', label='Full SBI')\n",
    "\n",
    "ax.legend(\n",
    "    handles=[minLS_patch,minSBI_patch,fullLS_patch,fullSBI_patch],\n",
    "    loc='lower left',         # base location  # fine-tune the legend's position\n",
    "    frameon=False, ncols=2,\n",
    "    bbox_to_anchor=(0.12, 0.8),fontsize=24,\n",
    "    columnspacing=0.5,\n",
    "    handlelength=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f42acb-203a-4848-8bfa-667c1c95915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pos = np.array([0, 2, 4,6])*2\n",
    "POSITIONS = g_pos\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors[:,:,-1],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='turquoise'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors[:,:,-1].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors[:,:,-1]):\n",
    "    ax.scatter(x, y, s = 100, color='paleturquoise', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+0.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors_Min[:,:,-1],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='cadetblue'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors_Min[:,:,-1].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors_Min[:,:,-1]):\n",
    "    ax.scatter(x, y, s = 100, color='darkturquoise', alpha=0.8)\n",
    "\n",
    "\n",
    "POSITIONS = g_pos+1.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors[:,:,-1],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='sandybrown'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors[:,:,-1].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors[:,:,-1]):\n",
    "    ax.scatter(x, y, s = 100, color='peachpuff', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+2\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors_Min[:,:,-1],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='darkorange'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors_Min[:,:,-1].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors_Min[:,:,-1]):\n",
    "    ax.scatter(x, y, s = 100, color='orange', alpha=0.8)\n",
    "\n",
    "ax.set_xticks([1,5,9,13],['2','10','20','30'],fontsize=24)\n",
    "ax.set_xlabel('SNR',fontsize=32)\n",
    "ax.tick_params(axis='x', labelsize=24)\n",
    "ax.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax.tick_params(axis='y', labelsize=24,)\n",
    "ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "\n",
    "minLS_patch = mpatches.Patch(color='darkorange', label='Minimum NLLS')\n",
    "fullLS_patch = mpatches.Patch(color='sandybrown', label='Full NLLS')\n",
    "\n",
    "minSBI_patch = mpatches.Patch(color='lightseagreen', label='Minimum SBI')\n",
    "fullSBI_patch = mpatches.Patch(color='paleturquoise', label='Full SBI')\n",
    "\n",
    "plt.ylim([-0.1,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c00655-fd03-4003-b48d-f0741863974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pos = np.array([0, 2, 4,6])*2\n",
    "POSITIONS = g_pos\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors[:,:,-4],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='turquoise'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors[:,:,-4].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors[:,:,-4]):\n",
    "    ax.scatter(x, y, s = 100, color='paleturquoise', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+0.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors_Min[:,:,-4],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='cadetblue'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors_Min[:,:,-4].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors_Min[:,:,-4]):\n",
    "    ax.scatter(x, y, s = 100, color='darkturquoise', alpha=0.8)\n",
    "\n",
    "\n",
    "POSITIONS = g_pos+1.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors[:,:,-4],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='sandybrown'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors[:,:,-4].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors[:,:,-4]):\n",
    "    ax.scatter(x, y, s = 100, color='peachpuff', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+2\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors_Min[:,:,-4],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='darkorange'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors_Min[:,:,-4].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors_Min[:,:,-4]):\n",
    "    ax.scatter(x, y, s = 100, color='orange', alpha=0.8)\n",
    "\n",
    "ax.set_xticks([1,5,9,13],['2','10','20','30'],fontsize=24)\n",
    "ax.set_xlabel('SNR',fontsize=32)\n",
    "ax.tick_params(axis='x', labelsize=24)\n",
    "ax.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax.tick_params(axis='y', labelsize=24,)\n",
    "ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "\n",
    "minLS_patch = mpatches.Patch(color='darkorange', label='Minimum NLLS')\n",
    "fullLS_patch = mpatches.Patch(color='sandybrown', label='Full NLLS')\n",
    "\n",
    "minSBI_patch = mpatches.Patch(color='lightseagreen', label='Minimum SBI')\n",
    "fullSBI_patch = mpatches.Patch(color='paleturquoise', label='Full SBI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc8edd9-ba2f-4db6-8075-04dc511a2ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pos = np.array([0, 2, 4,6])*2\n",
    "POSITIONS = g_pos\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors[:,:,-2],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,4))\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='turquoise'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors[:,:,-2].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors[:,:,-2]):\n",
    "    ax.scatter(x, y, s = 100, color='paleturquoise', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+0.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(SBI_Errors_Min[:,:,-2],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='cadetblue'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    SBI_Errors_Min[:,:,-2].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, SBI_Errors_Min[:,:,-2]):\n",
    "    ax.scatter(x, y, s = 100, color='darkturquoise', alpha=0.8)\n",
    "\n",
    "\n",
    "POSITIONS = g_pos+1.5\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors[:,:,-2],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='sandybrown'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors[:,:,-2].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors[:,:,-2]):\n",
    "    ax.scatter(x, y, s = 100, color='peachpuff', alpha=0.8)\n",
    "\n",
    "POSITIONS = g_pos+2\n",
    "\n",
    "jitter = 0.04\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(LS_Errors_Min[:,:,-2],)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='darkorange'\n",
    ")\n",
    "\n",
    "ax.boxplot(\n",
    "    LS_Errors_Min[:,:,-2].T,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Add jittered dots ----------------------------------------------\n",
    "for x, y in zip(x_jittered, LS_Errors_Min[:,:,-2]):\n",
    "    ax.scatter(x, y, s = 100, color='orange', alpha=0.8)\n",
    "\n",
    "ax.set_xticks([1,5,9,13],['2','10','20','30'],fontsize=24)\n",
    "ax.set_xlabel('SNR',fontsize=32)\n",
    "ax.tick_params(axis='x', labelsize=24)\n",
    "ax.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax.tick_params(axis='y', labelsize=24,)\n",
    "ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "\n",
    "minLS_patch = mpatches.Patch(color='darkorange', label='Minimum NLLS')\n",
    "fullLS_patch = mpatches.Patch(color='sandybrown', label='Full NLLS')\n",
    "\n",
    "minSBI_patch = mpatches.Patch(color='lightseagreen', label='Minimum SBI')\n",
    "fullSBI_patch = mpatches.Patch(color='paleturquoise', label='Full SBI')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0406d-44a8-4666-b1c7-4fd40d00aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('say Finished this part')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b221a2-de77-4b53-a3d4-5130181d0ea8",
   "metadata": {},
   "source": [
    "# Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8fc53-2ad7-4eed-9c45-ffc999c0e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymatreader as pmt\n",
    "import matplotlib.pyplot as plt\n",
    "from dipy.segment.mask import median_otsu\n",
    "\n",
    "from dipy.io.image import load_nifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9afb8-6dd7-4f60-8b4b-239a6c5ef7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dir = '/Users/maximilianeggl/Dropbox/PostDoc/Silvia/HealthyCat/done/Ctrl055_R01_28/'\n",
    "dat = pmt.read_mat(Dir+'data_loaded.mat')\n",
    "bvecs = dat['direction']\n",
    "bvals = dat['bval']\n",
    "FixedParams = {\n",
    "    'bvals':bvals,\n",
    "    'bvecs':bvecs,\n",
    "    'Delta':[0.017,0.035,0.061],\n",
    "    'delta':0.007,\n",
    "}\n",
    "Delta = FixedParams['Delta']\n",
    "delta = FixedParams['delta']\n",
    "n_pts = 90\n",
    "\n",
    "S_mask, _, _ = load_nifti(Dir+'mask_055.nii.gz', return_img=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87df99d7-85c1-45b9-8492-76d29d079a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dat['data']\n",
    "axial_middle = data.shape[2] // 2\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(0, 10), median_radius=5,\n",
    "                             numpass=1, autocrop=False, dilate=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c994a-c4ae-49a0-a141-18a6a19c7153",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(maskdata[:,:,41,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac2b527-4720-4dba-a22d-9b3879fb34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.flipud(maskdata[:,51,:,0].T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c2c8f1-2e0b-41be-af57-0386fe4c2d6e",
   "metadata": {},
   "source": [
    "## SBI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b6d5f4-5b88-449d-a446-5a4e6314b7e3",
   "metadata": {},
   "source": [
    "### Full Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8850ad-9efd-4e5c-b581-5a5e735450e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "NumSamps = 200000\n",
    "\n",
    "# Directions\n",
    "x1  = np.random.randn(NumSamps)\n",
    "y1  = np.random.randn(NumSamps)\n",
    "z1  =  np.random.randn(NumSamps)\n",
    "V = np.vstack([x1,y1,z1])\n",
    "V = (V/np.linalg.norm(V,axis=0)).T\n",
    "Angs = np.array([SpherAng(v) for v in V])\n",
    "\n",
    "#Diffusion of restricted\n",
    "Dpar  = np.random.rand(NumSamps)*5e-3\n",
    "Dperp = np.random.rand(NumSamps)*5e-3\n",
    "\n",
    "#Diffusion of hindered\n",
    "Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "\n",
    "#Fraction of hindered\n",
    "frac  = np.random.rand(NumSamps)\n",
    "\n",
    "mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "\n",
    "S0Rand =np.random.rand(NumSamps)*2475+25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b24d6-19ee-4ca5-bb69-356432449d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainParams = np.column_stack([V,Angs,Dpar,Dperp,DHind,frac,mean,S0Rand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d917154-2cb9-4cf6-bf33-e6772efd2d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/Full_Dat_50_200k_poisson.pickle\"):\n",
    "    with open(f\"{network_path}/Full_Dat_50_200k_poisson.pickle\", \"rb\") as handle:\n",
    "        posterior = pickle.load(handle)\n",
    "else:\n",
    "\n",
    "    \n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "        \n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "    \n",
    "        a = mean[i]\n",
    "        s0 = S0Rand[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "    \n",
    "        TrainSig1 = CombSignal_poisson(bvecs[:(n_pts+1)],bvals[:(n_pts+1)],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(bvecs[(n_pts+1):2*(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(bvecs[2*(n_pts+1):],bvals[2*(n_pts+1):],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(AddNoise(TrainSig[-1],s0,Noise))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "\n",
    "\n",
    "\n",
    "    Obs = torch.tensor(NoisyTrainSig).float()\n",
    "    Par = torch.tensor(TrainParams[:,3:]).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs=100)\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    with open(f\"{network_path}/Full_Dat_50_200k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa38d39-b40f-4e03-aa10-79296fbf4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, 54, :, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel(i, j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posterior.sample((1000,), x=maskdata[i, 54, j, :],show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    NoiseEst[i, j,-2] = np.clip(NoiseEst[i, j,-2],0,100)\n",
    "    NoiseEst[i, j,-3] = np.clip(NoiseEst[i, j,-3],0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d97a7-c700-4ba9-92a1-d8392518b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 = np.copy(NoiseEst)\n",
    "\n",
    "mask1 = np.ones_like(S_mask[:,54,:])\n",
    "mask1[S_mask[:,54,:]==0] = 0\n",
    "structure = np.ones((3, 3), dtype=bool)\n",
    "\n",
    "# Apply dilation. Increase 'iterations' to make the mask even fatter.\n",
    "fat_mask = binary_dilation(mask1, structure=structure, iterations=1)\n",
    "\n",
    "comb_mask = fat_mask * ((1-NoiseEst2[...,-3])>0.1)\n",
    "\n",
    "mask_CC = (1-NoiseEst2[...,-3])<0.3\n",
    "for i in range(13):\n",
    "    NoiseEst2[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2[~comb_mask,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550dcb77-7d83-4466-982a-884eb3c3ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 = np.copy(NoiseEst)\n",
    "mask_CC = (1-NoiseEst2[...,-3])<0.3\n",
    "for i in range(13):\n",
    "    NoiseEst2[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2[~comb_mask,-2] = math.nan\n",
    "plt.subplots(figsize=(12,12))\n",
    "plt.imshow(np.flipud(NoiseEst2[...,-1].T),cmap='gray')\n",
    "im = plt.imshow(np.flipud(NoiseEst2[...,-2].T),cmap='hot',vmin=0,vmax=0.005)\n",
    "cbar = plt.colorbar(im,fraction=0.035, pad=0.01,format=ticker.FormatStrFormatter('%.e'))\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da273db-80e8-462d-bd99-3eba3d8ba853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel(i, j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posterior.sample((1000,), x=maskdata[i, j,axial_middle, :],show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    NoiseEst[i, j,-2] = np.clip(NoiseEst[i, j,-2],0,100)\n",
    "    NoiseEst[i, j,-3] = np.clip(NoiseEst[i, j,-3],0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773220e1-baad-4fda-a465-900ee136f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2 = np.copy(NoiseEst)\n",
    "\n",
    "for i in range(13):\n",
    "    NoiseEst2[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2[(1-NoiseEst2[...,-3])<0.3,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a36a964-920e-42e4-9640-f744dbecaf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "im = plt.imshow(1-NoiseEst2[...,-3],vmin=0,vmax=1,cmap='hot')\n",
    "cbar = plt.colorbar(im,fraction=0.035, pad=-0.1)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79469cc-05d0-4929-a1c9-6d0f6505afb7",
   "metadata": {},
   "source": [
    "### Min Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bddccd-d593-466b-951b-fff75b5a69b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "NumSamps = 10000\n",
    "\n",
    "# Directions\n",
    "x1  = np.random.randn(NumSamps)\n",
    "y1  = np.random.randn(NumSamps)\n",
    "z1  =  np.random.randn(NumSamps)\n",
    "V = np.vstack([x1,y1,z1])\n",
    "V = (V/np.linalg.norm(V,axis=0)).T\n",
    "Angs = np.array([SpherAng(v) for v in V])\n",
    "\n",
    "#Diffusion of restricted\n",
    "Dpar  = np.random.rand(NumSamps)*5e-3\n",
    "Dperp = np.random.rand(NumSamps)*5e-3\n",
    "\n",
    "#Diffusion of hindered\n",
    "Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "\n",
    "#Fraction of hindered\n",
    "frac  = np.random.rand(NumSamps)\n",
    "\n",
    "mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "scale = np.random.rand(NumSamps)*0.0009+0.0001\n",
    "\n",
    "S0Rand =np.random.rand(NumSamps)*2475+25\n",
    "TrainParams = np.column_stack([V,Angs,Dpar,Dperp,DHind,frac,mean,S0Rand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4da5f-833c-47d7-a2ad-8ac67f9eabac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs2000 = bvecs[:91][bvals[:91]==2000]\n",
    "distance_matrix = squareform(pdist(bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs2000_selected = bvecs[:91][bvals[:91]==2000][selected_indices]\n",
    "true_indices = []\n",
    "for b in bvecs2000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs4000 = bvecs[:91][bvals[:91]==4000]\n",
    "distance_matrix = squareform(pdist(bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs4000_selected = bvecs[:91][bvals[:91]==4000][selected_indices]\n",
    "for b in bvecs4000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "true_indices1 = true_indices\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs2000 = bvecs[91:182][bvals[91:182]==2000]\n",
    "distance_matrix = squareform(pdist(bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs2000_selected = bvecs[91:182][bvals[91:182]==2000][selected_indices]\n",
    "true_indices = []\n",
    "for b in bvecs2000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs4000 = bvecs[91:182][bvals[91:182]==4000]\n",
    "distance_matrix = squareform(pdist(bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs4000_selected = bvecs[91:182][bvals[91:182]==4000][selected_indices]\n",
    "for b in bvecs4000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "true_indices2 = true_indices\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs2000 = bvecs[182:][bvals[182:]==2000]\n",
    "distance_matrix = squareform(pdist(bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs2000_selected = bvecs[182:][bvals[182:]==2000][selected_indices]\n",
    "true_indices = []\n",
    "for b in bvecs2000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs4000 = bvecs[182:][bvals[182:]==4000]\n",
    "distance_matrix = squareform(pdist(bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs4000_selected = bvecs[182:][bvals[182:]==4000][selected_indices]\n",
    "for b in bvecs4000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "true_indices3 = true_indices\n",
    "\n",
    "DevIndices = [0] + true_indices1 + true_indices2 + true_indices3\n",
    "bvecs_Dev = bvecs[DevIndices]\n",
    "bvals_Dev = bvals[DevIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18262a3-2e1e-4f9a-8aa7-fe1131f41e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "NumSamps = 100000\n",
    "\n",
    "# Directions\n",
    "x1  = np.random.randn(NumSamps)\n",
    "y1  = np.random.randn(NumSamps)\n",
    "z1  =  np.random.randn(NumSamps)\n",
    "V = np.vstack([x1,y1,z1])\n",
    "V = (V/np.linalg.norm(V,axis=0)).T\n",
    "Angs = np.array([SpherAng(v) for v in V])\n",
    "\n",
    "#Diffusion of restricted\n",
    "Dpar  = np.random.rand(NumSamps)*5e-3\n",
    "Dperp = np.random.rand(NumSamps)*5e-3\n",
    "\n",
    "#Diffusion of hindered\n",
    "Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "\n",
    "#Fraction of hindered\n",
    "frac  = np.random.rand(NumSamps)\n",
    "\n",
    "mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "scale = np.random.rand(NumSamps)*0.0009+0.0001\n",
    "\n",
    "S0Rand =np.random.rand(NumSamps)*2475+25\n",
    "TrainParams = np.column_stack([V,Angs,Dpar,Dperp,DHind,frac,mean,S0Rand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69595c10-a406-448b-87b4-516ccfb2195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/Dev_Dat_50_200k_poisson.pickle\"):\n",
    "    with open(f\"{network_path}/Dev_Dat_50_200k_poisson.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "    \n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "        \n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "    \n",
    "        a = mean[i]\n",
    "        #s = sig2[i]\n",
    "        s0 = S0Rand[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "                \n",
    "        TrainSig1 = CombSignal_poisson(bvecs_Dev[:7],bvals_Dev[:7],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(bvecs_Dev[7:13],bvals_Dev[7:13],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(bvecs_Dev[13:],bvals_Dev[13:],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(AddNoise(TrainSig[-1],s0,Noise))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "\n",
    "\n",
    "    Obs = torch.tensor(NoisyTrainSig).float()\n",
    "    Par = torch.tensor(TrainParams[:,3:]).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train()\n",
    "    posteriorMin = inference.build_posterior(density_estimator)\n",
    "    with open(f\"{network_path}/Dev_Dat_50_200k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posteriorMin, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610cd8e1-8e0d-44ad-8ed4-3cecec7f64e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, 54, :, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel(i, j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posteriorMin.sample((500,), x=maskdata[i, 54, j, DevIndices],show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_Min = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_Min[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    NoiseEst_Min[i, j,-2] = np.clip(NoiseEst_Min[i, j,-2],0,100)\n",
    "    NoiseEst_Min[i, j,-3] = np.clip(NoiseEst_Min[i, j,-3],0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b717cf8a-338d-4405-b9b4-f148e42deeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2_Min = np.copy(NoiseEst_Min)\n",
    "\n",
    "for i in range(13):\n",
    "    NoiseEst2_Min[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2_Min[~comb_mask,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cab89d-d996-481c-ac16-8dae1d234108",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "plt.imshow(np.flipud(NoiseEst2_Min[...,-1].T),cmap='gray')\n",
    "im = plt.imshow(np.flipud(NoiseEst2_Min[...,-2].T),cmap='hot',vmin=0,vmax=0.005)\n",
    "cbar = plt.colorbar(im,fraction=0.035, pad=0.01,format=ticker.FormatStrFormatter('%.e'))\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c84398e-ad06-4a52-b63e-918ed8ce5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48643998-3363-461e-9010-27a73ac15208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel(i, j):\n",
    "    torch.manual_seed(10)  # If required\n",
    "    posterior_samples_1 = posteriorMin.sample((1000,), x=maskdata[i, j,axial_middle, DevIndices],show_progress_bars=False)\n",
    "    return i, j, posterior_samples_1.mean(axis=0)\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_Min = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_Min[i, j] = x\n",
    "\n",
    "for i, j, x in results:\n",
    "    NoiseEst_Min[i, j,-2] = np.clip(NoiseEst_Min[i, j,-2],0,100)\n",
    "    NoiseEst_Min[i, j,-3] = np.clip(NoiseEst_Min[i, j,-3],0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b637b558-3768-4a3c-909f-83086a20e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "im = plt.imshow(1-NoiseEst_Min[...,-3],cmap='hot',vmin=0,vmax=1)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82670ee3-7d4b-4a28-9937-059f8fe7543c",
   "metadata": {},
   "source": [
    "## NLLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980387f7-1326-47a6-9c4c-223586022b21",
   "metadata": {},
   "source": [
    "### Full Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c51f6-824e-40f4-bd47-cae4f03a27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(133)\n",
    "S0 = 2000\n",
    "mean_guess = np.random.rand()*0.005+1e-4\n",
    "Params_abc =  np.random.rand(1,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(1,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind_guess = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind_guess = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind_guess])\n",
    "\n",
    "Dpar_guess = np.random.rand()*1e-3            # mm^2/s\n",
    "Dperp_guess = np.random.rand()*1e-3             # mm^2/s\n",
    "phi = 0#np.random.rand()*pi\n",
    "cos_theta = 0#np.random.rand()  # uniform in [0,1]\n",
    "theta = np.arccos(cos_theta)         # in [0, pi/2]\n",
    "Angs_guess = np.vstack([theta,phi]).T\n",
    "S0_guess =np.random.rand()*2475+25\n",
    "\n",
    "frac_guess = np.random.rand()\n",
    "guess = np.column_stack([Angs_guess,Dpar_guess,Dperp_guess,DHind_guess,frac_guess,mean_guess,S0_guess]).squeeze()\n",
    "bounds = np.array([[-np.inf,np.inf]]*13).T\n",
    "bounds[:,0] = [0,np.pi/2]\n",
    "bounds[:,1] = [-np.pi,np.pi]\n",
    "bounds[:,2] = [0,5e-3]\n",
    "bounds[:,3] = [0,5e-3]\n",
    "bounds[:,4] = [-5e-3,5e-3]\n",
    "bounds[:,5] = [-5e-3,5e-3]\n",
    "bounds[:,6] = [-5e-3,5e-3]\n",
    "bounds[:,7] = [-5e-3,5e-3]\n",
    "bounds[:,8] = [-5e-3,5e-3]\n",
    "bounds[:,9] = [-5e-3,5e-3]\n",
    "bounds[:,10] = [0,1]\n",
    "bounds[:,11] = [1e-4,0.005+1e-4]\n",
    "bounds[:,12] = [25,2500]\n",
    "\n",
    "bve_split = [bvecs[:(n_pts+1)],bvecs[(n_pts+1):2*(n_pts+1)],bvecs[2*(n_pts+1):]]\n",
    "bva_split = [bvals[:(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],bvals[2*(n_pts+1):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eca294-fc3d-4633-8d4f-60a68e98a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, 54, :, :], axis=-1) != 0\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel_LS(i, j):\n",
    "    result = sp.optimize.least_squares(residuals_S0, guess, args=[maskdata[i, 54, j, :],bve_split,bva_split,Delta],\n",
    "                              bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "    return i, j, result.x\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_LS[i, j] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e5d64-01d1-4cd3-b647-d7d6b4e41d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2_LS = np.copy(NoiseEst_LS)\n",
    "\n",
    "for i in range(13):\n",
    "    NoiseEst2_LS[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2_LS[~comb_mask,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd5f61-99ac-4a62-bdba-55e357bf3b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "plt.imshow(np.flipud(NoiseEst2_LS[...,-1].T),cmap='gray')\n",
    "im = plt.imshow(np.flipud(NoiseEst2_LS[...,-2].T),cmap='hot',vmin=0,vmax=0.005)\n",
    "cbar = plt.colorbar(im,fraction=0.03, pad=0.01,format=ticker.FormatStrFormatter('%.e'))\n",
    "cbar.ax.tick_params(labelsize=32)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cbfff0-03cb-4d07-8748-fd587e279f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel_LS(i, j):\n",
    "    result = sp.optimize.least_squares(residuals_LS_real_dat, guess, args=[maskdata[i, j,axial_middle, :]],\n",
    "                              bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "    return i, j, result.x\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_LS[i, j] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf6ba8-5032-4c7e-b483-afb3cea5a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2_LS = np.copy(NoiseEst_LS)\n",
    "\n",
    "for i in range(13):\n",
    "    NoiseEst2_LS[~mask,i] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b0d13-99c2-4ad9-ae82-338a2df83ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "im = plt.imshow(1-NoiseEst2_LS[...,-3],vmin=0,vmax=1,cmap='hot')\n",
    "cbar = plt.colorbar(im,fraction=0.035, pad=-0.1)\n",
    "cbar.ax.tick_params(labelsize=32)\n",
    "plt.axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13373f1a-158f-4110-8f0e-a0a2f69439a1",
   "metadata": {},
   "source": [
    "### Min Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef06c5b-5177-4f8c-9147-ded30ecde826",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5#np.arange(2,28,2)[::-1][10]\n",
    "#Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs2000 = bvecs[:91][bvals[:91]==2000]\n",
    "distance_matrix = squareform(pdist(bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(k):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs2000_selected = bvecs[:91][bvals[:91]==2000][selected_indices]\n",
    "true_indices = []\n",
    "for b in bvecs2000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs4000 = bvecs[:91][bvals[:91]==4000]\n",
    "distance_matrix = squareform(pdist(bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(k):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs4000_selected = bvecs[:91][bvals[:91]==4000][selected_indices]\n",
    "for b in bvecs4000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "true_indices1 = true_indices\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs2000 = bvecs[91:182][bvals[91:182]==2000]\n",
    "distance_matrix = squareform(pdist(bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(k):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs2000_selected = bvecs[91:182][bvals[91:182]==2000][selected_indices]\n",
    "true_indices = []\n",
    "for b in bvecs2000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs4000 = bvecs[91:182][bvals[91:182]==4000]\n",
    "distance_matrix = squareform(pdist(bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(k):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs4000_selected = bvecs[91:182][bvals[91:182]==4000][selected_indices]\n",
    "for b in bvecs4000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "true_indices2 = true_indices\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs2000 = bvecs[182:][bvals[182:]==2000]\n",
    "distance_matrix = squareform(pdist(bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(k):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs2000_selected = bvecs[182:][bvals[182:]==2000][selected_indices]\n",
    "true_indices = []\n",
    "for b in bvecs2000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "\n",
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "selected_indices = [0]\n",
    "bvecs4000 = bvecs[182:][bvals[182:]==4000]\n",
    "distance_matrix = squareform(pdist(bvecs))\n",
    "# Iteratively select the point furthest from the current selection\n",
    "for _ in range(k):  # We need 7 points in total, and one is already selected\n",
    "    remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "    \n",
    "    # Calculate the minimum distance to the selected points for each remaining point\n",
    "    min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "    \n",
    "    # Select the point with the maximum minimum distance\n",
    "    next_index = remaining_indices[np.argmax(min_distances)]\n",
    "    selected_indices.append(next_index)\n",
    "\n",
    "selected_indices = selected_indices\n",
    "bvecs4000_selected = bvecs[182:][bvals[182:]==4000][selected_indices]\n",
    "for b in bvecs4000_selected:\n",
    "    true_indices.append(np.where((b == bvecs).all(axis=1))[0][0])\n",
    "true_indices3 = true_indices\n",
    "\n",
    "DevIndices = [0] + true_indices1 + true_indices2 + true_indices3\n",
    "print(len(DevIndices))\n",
    "bvecs_Dev = bvecs[DevIndices]\n",
    "bvals_Dev = bvals[DevIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd26b13-9ce6-4d6f-b799-af0e30dfb7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bvecs_Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b3498-e1ea-4b2d-9d99-5f5fd174783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bvals_Dev[13:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce48261-f5a1-4b6a-a802-9df86a135bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals_S0(params,TrueSig,bvecs,bvals,Delta):\n",
    "    Signal = Simulator(params,bvecs,bvals,Delta,S0=params[-1])\n",
    "    return TrueSig - Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27b26e-d910-4bf2-971d-429916286b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j = 0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ddaed-20bd-4213-ae9f-5eead8e8b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575bfca-4bd5-49f8-bbe5-19aea5d4018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.optimize.least_squares(residuals_S0, guess, args=[maskdata[i, j,axial_middle, DevIndices],[bvecs_Dev[:13],bvecs_Dev[13:25],bvecs_Dev[25:]]\n",
    "                                                                                              ,[bvals_Dev[:13],bvals_Dev[13:25],bvals_Dev[25:]],Delta],\n",
    "                          bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f7479-5c23-4f74-b6b2-fc771d38f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "args=[NoisyTestSig[j,i],bvecs_split,bvals_split,Delta],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0885306-1c83-4613-9927-1d15018a5121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, :, axial_middle, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel_LS(i, j):\n",
    "    result = sp.optimize.least_squares(residuals_S0, guess, args=[maskdata[i, j,axial_middle, DevIndices],[bvecs_Dev[:13],bvecs_Dev[13:25],bvecs_Dev[25:]]\n",
    "                                                                                              ,[bvals_Dev[:13],bvals_Dev[13:25],bvals_Dev[25:]],Delta],\n",
    "                          bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "    return i, j, result.x\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_LS_Min = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_LS_Min[i, j] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e2b98-c9c0-4295-99ee-2f8f9c573efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb58f4a3-b8fe-4530-8735-17d706724698",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DevIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b927e5-07d9-4862-b32e-a12b58305bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst_LS_Min = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_LS_Min[i, j] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d8344d-7ada-4741-a486-404a5ae048de",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2_LS_Min = np.copy(NoiseEst_LS_Min)\n",
    "\n",
    "#for i in range(13):\n",
    "#    NoiseEst2_LS_Min[~mask,i] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10bde94-ac37-441c-b27c-40eedf607b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -2\n",
    "score,ssim_map = ssim(NoiseEst2_LS_Min[...,i],NoiseEst[...,i], data_range=np.max(NoiseEst2_LS_Min[...,i])-np.min(NoiseEst_Min[...,i]),full=True)\n",
    "mask = np.zeros_like(ssim_map, dtype=bool)\n",
    "masked_ssim = ssim_map[Outlines[0][:, :, axial_middle]].mean()\n",
    "print(masked_ssim)\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(12,4))\n",
    "ax[0].imshow(1-NoiseEst[...,i],cmap='hot')\n",
    "ax[1].imshow(1-NoiseEst2_LS_Min[...,i],cmap='hot')\n",
    "ax[2].imshow(ssim_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64180bb7-3b92-4120-9e05-8d2464c3ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "im = plt.imshow(1-NoiseEst2_LS_Min[...,-3],cmap='hot')\n",
    "cbar = plt.colorbar(im,fraction=0.035, pad=-0.1)\n",
    "cbar.ax.tick_params(labelsize=32)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f1391-030f-4e29-87d1-ead8a2136ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mask where the sum is not zero\n",
    "mask = np.sum(maskdata[:, 54, :, :], axis=-1) != 0\n",
    "\n",
    "# Get the indices where mask is True\n",
    "indices = np.argwhere(mask)\n",
    "\n",
    "# Define the function for optimization\n",
    "def optimize_pixel_LS(i, j):\n",
    "    result = sp.optimize.least_squares(residuals_LS_real_dat_Min, guess, args=[maskdata[i,54,j, FullIndices]],\n",
    "                              bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "    return i, j, result.x\n",
    "\n",
    "# Initialize NoiseEst with the appropriate shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ArrShape = mask.shape\n",
    "\n",
    "# Use joblib to parallelize the optimization tasks\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    ")\n",
    "\n",
    "\n",
    "NoiseEst_LS_Min = np.zeros(list(ArrShape) + [13])\n",
    "\n",
    "# Assign the optimization results to NoiseEst\n",
    "for i, j, x in results:\n",
    "    NoiseEst_LS_Min[i, j] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec02ded-1276-4b13-b1d2-fb7eaaf6e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseEst2_LS_Min = np.copy(NoiseEst_LS_Min)\n",
    "\n",
    "for i in range(13):\n",
    "    NoiseEst2_LS_Min[~mask,i] = math.nan\n",
    "\n",
    "NoiseEst2_LS_Min[~comb_mask,-2] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01605792-9821-4f42-99e9-77744fbfc389",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Temp_LS_Min_CC.npy',NoiseEst_LS_Min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ef9d6-d2b8-4af9-9fa8-38b50e351e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "plt.imshow(np.flipud(NoiseEst2_LS_Min[...,-1].T),cmap='gray')\n",
    "im = plt.imshow(np.flipud(NoiseEst2_LS_Min[...,-2].T),cmap='hot',vmin=0,vmax=0.005)\n",
    "cbar = plt.colorbar(im,fraction=0.03, pad=0.01,format=ticker.FormatStrFormatter('%.e'))\n",
    "cbar.ax.tick_params(labelsize=32)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b606173b-61fc-45e9-8ba3-830f3be79d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,12))\n",
    "plt.imshow(np.flipud(NoiseEst2_LS_Min[...,-1].T),cmap='gray')\n",
    "im = plt.imshow(np.flipud(NoiseEst2_LS_Min[...,-2].T),cmap='hot',vmin=0,vmax=0.005)\n",
    "cbar = plt.colorbar(im,fraction=0.03, pad=0.01,format=ticker.FormatStrFormatter('%.e'))\n",
    "cbar.ax.tick_params(labelsize=32)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cac85ec-148e-4d87-8337-ec669240d6e1",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba563a-ba93-463f-bf65-d15e87497237",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBIf = NoiseEst2[comb_mask]\n",
    "SBIf2 = NoiseEst2_Min[comb_mask]\n",
    "SBIf_LS = NoiseEst2_LS[comb_mask]\n",
    "SBIf2_LS = NoiseEst2_LS_Min[comb_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b44214-48de-4dfc-ad18-69212859b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pos = np.array([0, 1, 2 ])*2\n",
    "POSITIONS = g_pos\n",
    "jitter = 0.04\n",
    "y_data = [abs(SBIf[:,-2]-SBIf2[:,-2]),abs(SBIf_LS[:,-2]-SBIf2_LS[:,-2]),abs(SBIf[:,-2]-SBIf_LS[:,-2])]\n",
    "x_data = [np.array([POSITIONS[i]] * len(d)) for i, d in enumerate(y_data,)]\n",
    "x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "colors = ['lightseagreen','darkorange','k']\n",
    "colors2 = ['paleturquoise','sandybrown','gray']\n",
    "fig,ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "# Customize violins (remove fill, customize line, etc.)\n",
    "for pc in violins[\"bodies\"]:\n",
    "    pc.set_facecolor(\"none\")\n",
    "    pc.set_edgecolor('k')\n",
    "    pc.set_linewidth(1)\n",
    "    pc.set_alpha(1)\n",
    "    \n",
    "\n",
    "# Add boxplots ---------------------------------------------------\n",
    "# Note that properties about the median and the box are passed\n",
    "# as dictionaries.\n",
    "\n",
    "medianprops = dict(\n",
    "    linewidth=2, \n",
    "    color=GREY_DARK,\n",
    "    solid_capstyle=\"butt\"\n",
    ")\n",
    "boxprops = dict(\n",
    "    linewidth=2, \n",
    "    color='turquoise'\n",
    ")\n",
    "\n",
    "bplot =  ax.boxplot(\n",
    "    y_data,\n",
    "    positions=POSITIONS, \n",
    "    showfliers = False, # Do not show the outliers beyond the caps.\n",
    "    showcaps = False,   # Do not show the caps\n",
    "    medianprops = medianprops,\n",
    "    whiskerprops = boxprops,\n",
    "    boxprops = boxprops\n",
    ")\n",
    "\n",
    "# Update the color of each box\n",
    "for i, box in enumerate(bplot['boxes']):\n",
    "    box.set_color(colors[i])\n",
    "    \n",
    "# Update the color of the medians\n",
    "for i, median in enumerate(bplot['medians']):\n",
    "    median.set_color(colors[i])\n",
    "    \n",
    "# Update the color of the whiskers.\n",
    "# Note: Each box has 2 whiskers, so they appear in order.\n",
    "for i in range(len(POSITIONS)):\n",
    "    bplot['whiskers'][2*i].set_color(colors[i])\n",
    "    bplot['whiskers'][2*i+1].set_color(colors[i])\n",
    "    \n",
    "# Optionally, update the color of the caps if you ever enable them\n",
    "if 'caps' in bplot:\n",
    "    for i, cap in enumerate(bplot['caps']):\n",
    "        cap.set_color(colors[i//2])  # since there are 2 caps per box\n",
    "\n",
    "for x, y,c in zip(x_jittered, y_data,colors2):\n",
    "    ax.scatter(x, y, s = 100, color=c, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acecfec7-464e-47ed-bf87-0b4397e3f659",
   "metadata": {},
   "source": [
    "# Three Indivs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5323a1-e100-48ba-9b7f-b0004ca77d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dirs = ['Ctrl055_R01_28','Ctrl056_R01_29','Ctrl057_R01_30']\n",
    "Masks = ['mask_055.nii.gz','mask_056.nii.gz','mask_057.nii.gz']\n",
    "BVecs = []\n",
    "BVals = []\n",
    "Deltas = []\n",
    "deltas = []\n",
    "S_masks = []\n",
    "Datas = []\n",
    "Outlines = []\n",
    "for D,M in tqdm(zip(Dirs,Masks)):\n",
    "    dat = pmt.read_mat('/Users/maximilianeggl/Dropbox/PostDoc/Silvia/HealthyCat/done/'+D+'/data_loaded.mat')\n",
    "    BVecs.append(dat['direction'])\n",
    "    BVals.append(dat['bval'])\n",
    "    Deltas.append(FixedParams['Delta'])\n",
    "    deltas.append(FixedParams['delta'])\n",
    "    \n",
    "    m, _, _ = load_nifti('/Users/maximilianeggl/Dropbox/PostDoc/Silvia/HealthyCat/done/'+D+'/'+M, return_img=True)\n",
    "    S_masks.append(m)\n",
    "\n",
    "    data = dat['data']\n",
    "    axial_middle = data.shape[2] // 2\n",
    "    md, mk = median_otsu(data, vol_idx=range(0, 10), median_radius=5,\n",
    "                                 numpass=1, autocrop=False, dilate=2)\n",
    "    Datas.append(md)\n",
    "    Outlines.append(mk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59b949f-e338-4e66-aa77-0b54b2ba7234",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "NumSamps = 600000\n",
    "\n",
    "# Directions\n",
    "x1  = np.random.randn(NumSamps)\n",
    "y1  = np.random.randn(NumSamps)\n",
    "z1  =  np.random.randn(NumSamps)\n",
    "V = np.vstack([x1,y1,z1])\n",
    "V = (V/np.linalg.norm(V,axis=0)).T\n",
    "Angs = np.array([SpherAng(v) for v in V])\n",
    "\n",
    "#Diffusion of restricted\n",
    "Dpar  = np.random.rand(NumSamps)*5e-3\n",
    "Dperp = np.random.rand(NumSamps)*5e-3\n",
    "\n",
    "#Diffusion of hindered\n",
    "Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "\n",
    "#Fraction of hindered\n",
    "frac  = np.random.rand(NumSamps)\n",
    "\n",
    "mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "\n",
    "S0Rand =np.random.rand(NumSamps)*2475+25\n",
    "\n",
    "Choice = np.random.choice([1,2,3],NumSamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae962c8-6476-4653-a7c0-330ecc452e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainParams = np.column_stack([V,Angs,Dpar,Dperp,DHind,frac,mean,S0Rand,Choice*100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3b186-ce0e-48a4-927f-4c285f480fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/3Indv_50_600k_poisson.pickle\"):\n",
    "    with open(f\"{network_path}/3Indv_50_600k_poisson.pickle\", \"rb\") as handle:\n",
    "        posterior = pickle.load(handle)\n",
    "else:\n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "        \n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "    \n",
    "        a = mean[i]\n",
    "        s0 = S0Rand[i]\n",
    "        c = Choice[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "    \n",
    "        TrainSig1 = CombSignal_poisson(BVecs[c-1][:(n_pts+1)],bvals[:(n_pts+1)],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(BVecs[c-1][(n_pts+1):2*(n_pts+1)],bvals[(n_pts+1):2*(n_pts+1)],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(BVecs[c-1][2*(n_pts+1):],bvals[2*(n_pts+1):],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(np.append(AddNoise(TrainSig[-1],s0,Noise),c*100))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Obs = torch.tensor(NoisyTrainSig).float()\n",
    "    Par = torch.tensor(TrainParams[:,3:]).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs=100)\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    with open(f\"{network_path}/3Indv_50_300k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e818f-dff3-4e7e-979d-5011aca35a33",
   "metadata": {},
   "source": [
    "## Minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1358d54-d542-488f-92de-c7587945ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "IndxArr  = []\n",
    "BVecsDev = []\n",
    "BValsDev = []\n",
    "for bve,bva in zip(BVecs,BVals): \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[:91][bva[:91]==2000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[:91][bva[:91]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[:91][bva[:91]==4000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[:91][bva[:91]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices1 = true_indices\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[91:182][bva[91:182]==2000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[91:182][bva[91:182]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[91:182][bva[91:182]==4000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[91:182][bva[91:182]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices2 = true_indices\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs2000 = bve[182:][bva[182:]==2000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs2000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs2000_selected = bve[182:][bva[182:]==2000][selected_indices]\n",
    "    true_indices = []\n",
    "    for b in bvecs2000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    \n",
    "    # Choose the first point (arbitrary starting point, e.g., the first gradient)\n",
    "    selected_indices = [0]\n",
    "    bvecs4000 = bve[182:][bva[182:]==4000]\n",
    "    distance_matrix = squareform(pdist(bve))\n",
    "    # Iteratively select the point furthest from the current selection\n",
    "    for _ in range(2):  # We need 7 points in total, and one is already selected\n",
    "        remaining_indices = list(set(range(len(bvecs4000))) - set(selected_indices))\n",
    "        \n",
    "        # Calculate the minimum distance to the selected points for each remaining point\n",
    "        min_distances = np.min(distance_matrix[remaining_indices][:, selected_indices], axis=1)\n",
    "        \n",
    "        # Select the point with the maximum minimum distance\n",
    "        next_index = remaining_indices[np.argmax(min_distances)]\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    selected_indices = selected_indices\n",
    "    bvecs4000_selected = bve[182:][bva[182:]==4000][selected_indices]\n",
    "    for b in bvecs4000_selected:\n",
    "        true_indices.append(np.where((b == bve).all(axis=1))[0][0])\n",
    "    true_indices3 = true_indices\n",
    "    \n",
    "    DevIndices = [0] + true_indices1 + true_indices2 + true_indices3\n",
    "    bvecs_Dev = bve[DevIndices]\n",
    "    bvals_Dev = bva[DevIndices]\n",
    "\n",
    "    IndxArr.append(DevIndices)\n",
    "    BVecsDev.append(bvecs_Dev)\n",
    "    BValsDev.append(bvals_Dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f6ee3-0c58-446e-ae50-a142e62bed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "NumSamps = 600000\n",
    "\n",
    "# Directions\n",
    "x1  = np.random.randn(NumSamps)\n",
    "y1  = np.random.randn(NumSamps)\n",
    "z1  =  np.random.randn(NumSamps)\n",
    "V = np.vstack([x1,y1,z1])\n",
    "V = (V/np.linalg.norm(V,axis=0)).T\n",
    "Angs = np.array([SpherAng(v) for v in V])\n",
    "\n",
    "#Diffusion of restricted\n",
    "Dpar  = np.random.rand(NumSamps)*5e-3\n",
    "Dperp = np.random.rand(NumSamps)*5e-3\n",
    "\n",
    "#Diffusion of hindered\n",
    "Params_abc =  np.random.rand(NumSamps,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(NumSamps,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind])\n",
    "\n",
    "#Fraction of hindered\n",
    "frac  = np.random.rand(NumSamps)\n",
    "\n",
    "mean = np.random.rand(NumSamps)*0.005+1e-4\n",
    "\n",
    "S0Rand =np.random.rand(NumSamps)*2475+25\n",
    "\n",
    "Choice = np.random.choice([1,2,3],NumSamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26e0d4-c723-45a7-a203-8a0d1efe4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainParams = np.column_stack([V,Angs,Dpar,Dperp,DHind,frac,mean,S0Rand,Choice*100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5324808-7610-4356-a405-1385531ec67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    V_angles = np.array([Angs[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c356b7-0322-4565-a6bc-1b4997b1f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{network_path}/Dev_3Indv_50_600k_poisson.pickle\"):\n",
    "    with open(f\"{network_path}/Dev_3Indv_50_600k_poisson.pickle\", \"rb\") as handle:\n",
    "        posteriorMin = pickle.load(handle)\n",
    "else:\n",
    "    TrainSig = []\n",
    "    NoisyTrainSig = []\n",
    "    for i in tqdm(range(NumSamps)):\n",
    "        v = np.array([Angs[i]])\n",
    "        dpar = Dpar[i]\n",
    "        dperp = Dperp[i]\n",
    "        \n",
    "        dh   = DHind[i]\n",
    "        f    = [frac[i],1-frac[i]]\n",
    "    \n",
    "        a = mean[i]\n",
    "        #s = sig2[i]\n",
    "        s0 = S0Rand[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "        c = Choice[i]\n",
    "        \n",
    "        Noise = 50#np.random.rand()*30 + 20\n",
    "\n",
    "        TrainSig1 = CombSignal_poisson(BVecsDev[c-1][:7],BValsDev[c-1][:7],Delta[0],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig2 = CombSignal_poisson(BVecsDev[c-1][7:13],BValsDev[c-1][7:13],Delta[1],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig3 = CombSignal_poisson(BVecsDev[c-1][13:],BValsDev[c-1][13:],Delta[2],delta,[v,dpar,dperp,dh,f,a,s0])\n",
    "        TrainSig.append(np.hstack([TrainSig1,TrainSig2,TrainSig3]))\n",
    "        NoisyTrainSig.append(np.append(AddNoise(TrainSig[-1],s0,Noise),c*100))\n",
    "    NoisyTrainSig = np.array(NoisyTrainSig)\n",
    "    \n",
    "    \n",
    "    Obs = torch.tensor(NoisyTrainSig).float()\n",
    "    Par = torch.tensor(TrainParams[:,3:]).float()\n",
    "    # Create inference object. Here, NPE is used.\n",
    "    inference = SNPE()\n",
    "    \n",
    "    # generate simulations and pass to the inference object\n",
    "    inference = inference.append_simulations(Par, Obs)\n",
    "    \n",
    "    # train the density estimator and build the posterior\n",
    "    density_estimator = inference.train(stop_after_epochs=100)\n",
    "    posteriorMin = inference.build_posterior(density_estimator)\n",
    "    with open(f\"{network_path}/Dev_3Indv_50_600k_poisson.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(posteriorMin, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a02b38-e49d-4004-811c-f210e1b87cbc",
   "metadata": {},
   "source": [
    "## Evaluation of CC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4176d767-4689-4de3-8d57-c053bb3477ee",
   "metadata": {},
   "source": [
    "### SBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bff9b1-cd6a-4fb3-b3e7-8e45ca76c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_SBI = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[54,52,54],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = sma[:,sl,:]\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = posterior.sample((1000,), x=np.append(D[i, sl, j, :],100*(kk+1)),show_progress_bars=False)\n",
    "        return i, j, posterior_samples_1.mean(axis=0)\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [14])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "\n",
    "    Full_SBI.append(NoiseEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc23183-6318-493f-ab46-d71176c3f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_SBI = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[54,52,54],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = sma[:,sl,:]\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = posteriorMin.sample((1000,), x=np.append(D[i, sl, j, IndxArr[kk]],100*(kk+1)),show_progress_bars=False)\n",
    "        return i, j, posterior_samples_1.mean(axis=0)\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [14])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "\n",
    "    Min_SBI.append(NoiseEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c81cfde-dfc7-4951-a431-e7eca13170a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMasks = []\n",
    "kk = 0\n",
    "d  = 54\n",
    "NoiseEst2 = np.copy(Full_SBI[kk])\n",
    "for i in range(14):\n",
    "    NoiseEst2[~Outlines[kk][:,d,:],i] = math.nan\n",
    "    \n",
    "mask1 = np.ones_like(S_masks[kk][:,d,:])\n",
    "mask1[S_masks[kk][:,d,:]==0] = 0\n",
    "structure = np.ones((3, 3), dtype=bool)\n",
    "\n",
    "# Apply dilation. Increase 'iterations' to make the mask even fatter.\n",
    "fat_mask = binary_dilation(mask1, structure=structure, iterations=1)\n",
    "\n",
    "CMasks.append(fat_mask * ((1-NoiseEst2[...,-4])>0.1) * (NoiseEst2[...,-4]>0))\n",
    "\n",
    "kk = 1\n",
    "d  = 52\n",
    "NoiseEst2 = np.copy(Full_SBI[kk])\n",
    "for i in range(14):\n",
    "    NoiseEst2[~Outlines[kk][:,d,:],i] = math.nan\n",
    "    \n",
    "mask1 = np.ones_like(S_masks[kk][:,d,:])\n",
    "mask1[S_masks[kk][:,d,:]==0] = 0\n",
    "structure = np.ones((3, 3), dtype=bool)\n",
    "\n",
    "# Apply dilation. Increase 'iterations' to make the mask even fatter.\n",
    "fat_mask = binary_dilation(mask1, structure=structure, iterations=1)\n",
    "\n",
    "CMasks.append(fat_mask * ((1-NoiseEst2[...,-4])>0) * (NoiseEst2[...,-4]>0))\n",
    "\n",
    "kk = 2\n",
    "d  = 54\n",
    "NoiseEst2 = np.copy(Full_SBI[kk])\n",
    "for i in range(14):\n",
    "    NoiseEst2[~Outlines[kk][:,d,:],i] = math.nan\n",
    "    \n",
    "mask1 = np.ones_like(S_masks[kk][:,d,:])\n",
    "mask1[S_masks[kk][:,d,:]==0] = 0\n",
    "structure = np.ones((3, 3), dtype=bool)\n",
    "\n",
    "# Apply dilation. Increase 'iterations' to make the mask even fatter.\n",
    "fat_mask = binary_dilation(mask1, structure=structure, iterations=1)\n",
    "CMasks.append(fat_mask * ((1-NoiseEst2[...,-4])>0.3) * (NoiseEst2[...,-4]>0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b380e-3a69-4102-ba5f-cc80dfa16998",
   "metadata": {},
   "source": [
    "### NLLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7417de92-dc8b-49cb-89c6-f441a00d278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(133)\n",
    "S0 = 2000\n",
    "mean_guess = np.random.rand()*0.005+1e-4\n",
    "Params_abc =  np.random.rand(1,3)*0.14-0.07\n",
    "Params_rest =  np.random.rand(1,3)*0.03-0.015\n",
    "Params = np.hstack([Params_abc,Params_rest])\n",
    "DHind_guess = np.array([ComputeDTI(p) for p in Params])\n",
    "DHind_guess = np.array([mat_to_vals(ForceLowFA(dt)) for dt in DHind_guess])\n",
    "\n",
    "Dpar_guess = np.random.rand()*1e-3            # mm^2/s\n",
    "Dperp_guess = np.random.rand()*1e-3             # mm^2/s\n",
    "phi = 0#np.random.rand()*pi\n",
    "cos_theta = 0#np.random.rand()  # uniform in [0,1]\n",
    "theta = np.arccos(cos_theta)         # in [0, pi/2]\n",
    "Angs_guess = np.vstack([theta,phi]).T\n",
    "S0_guess =np.random.rand()*2475+25\n",
    "\n",
    "frac_guess = np.random.rand()\n",
    "guess = np.column_stack([Angs_guess,Dpar_guess,Dperp_guess,DHind_guess,frac_guess,mean_guess,S0_guess]).squeeze()\n",
    "bounds = np.array([[-np.inf,np.inf]]*13).T\n",
    "bounds[:,0] = [0,np.pi/2]\n",
    "bounds[:,1] = [-np.pi,np.pi]\n",
    "bounds[:,2] = [0,5e-3]\n",
    "bounds[:,3] = [0,5e-3]\n",
    "bounds[:,4] = [-5e-3,5e-3]\n",
    "bounds[:,5] = [-5e-3,5e-3]\n",
    "bounds[:,6] = [-5e-3,5e-3]\n",
    "bounds[:,7] = [-5e-3,5e-3]\n",
    "bounds[:,8] = [-5e-3,5e-3]\n",
    "bounds[:,9] = [-5e-3,5e-3]\n",
    "bounds[:,10] = [0,1]\n",
    "bounds[:,11] = [1e-4,0.005+1e-4]\n",
    "bounds[:,12] = [25,2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ea26b-dddb-4b0e-8c8a-63d534421dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_LS = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[54,52,54],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = sma[:,sl,:]\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    bve_split = [BVecs[kk][:(n_pts+1)],BVecs[kk][(n_pts+1):2*(n_pts+1)],BVecs[kk][2*(n_pts+1):]]\n",
    "    bva_split = [BVals[kk][:(n_pts+1)],BVals[kk][(n_pts+1):2*(n_pts+1)],BVals[kk][2*(n_pts+1):]]\n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel_LS(i, j):\n",
    "        result = sp.optimize.least_squares(residuals_S0, guess, args=[D[i, sl, j, :],bve_split,bva_split,Delta],\n",
    "                                  bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        return i, j, result.x\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst_LS[i, j] = x\n",
    "\n",
    "    Full_LS.append(NoiseEst_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6728cf6-488d-47ae-b1f9-0ce0718a81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_LS = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[54,52,54],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = sma[:,sl,:]\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    bve_splitd = [BVecsDev[kk][:7],BVecsDev[kk][7:13],BVecsDev[kk][13:]]\n",
    "    bva_splitd = [BValsDev[kk][:7],BValsDev[kk][7:13],BValsDev[kk][13:]]\n",
    "\n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel_LS(i, j):\n",
    "        result = sp.optimize.least_squares(residuals_S0, guess, args=[D[i, sl, j, IndxArr[kk]],bve_splitd,bva_splitd,Delta],\n",
    "                                  bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        return i, j, result.x\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst_LS[i, j] = x\n",
    "\n",
    "    Min_LS.append(NoiseEst_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40427471-78e5-4c85-ac41-92fd64fbd32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoxPlots(y_data, positions, colors, colors2, ax,hatch = False):\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "\n",
    "    jitter = 0.02\n",
    "    x_data = [np.array([positions[i]] * len(d)) for i, d in enumerate(y_data)]\n",
    "    x_jittered = [x + stats.t(df=6, scale=jitter).rvs(len(x)) for x in x_data]\n",
    "\n",
    "    # Define properties for the boxes (patch objects)\n",
    "    boxprops = dict(\n",
    "        linewidth=2, \n",
    "        facecolor='none',       # use facecolor for filling (set to 'none' if you want no fill)\n",
    "        edgecolor='turquoise'   # edgecolor for the outline\n",
    "    )\n",
    "\n",
    "    # Define properties for the medians (Line2D objects)\n",
    "    # Ensure GREY_DARK is defined (or replace it with a color string)\n",
    "    medianprops = dict(\n",
    "        linewidth=2, \n",
    "        color='dimgray',  # Replace 'GREY_DARK' with an actual color if needed\n",
    "        solid_capstyle=\"butt\"\n",
    "    )\n",
    "\n",
    "    # For whiskers, since they are Line2D objects, use 'color'\n",
    "    whiskerprops = dict(\n",
    "        linewidth=2, \n",
    "        color='turquoise'\n",
    "    )\n",
    "\n",
    "    bplot = ax.boxplot(\n",
    "        y_data,\n",
    "        positions=positions, \n",
    "        showfliers=False,\n",
    "        showcaps=False,\n",
    "        showmeans=True,\n",
    "        medianprops=medianprops,\n",
    "        whiskerprops=whiskerprops,\n",
    "        boxprops=boxprops,\n",
    "        patch_artist=True\n",
    "    )\n",
    "\n",
    "    # Update the color of each box (these are patch objects)\n",
    "    for i, box in enumerate(bplot['boxes']):\n",
    "        box.set_edgecolor(colors[i])\n",
    "        if(hatch):\n",
    "            box.set_hatch('/')\n",
    "    \n",
    "    # Update the color of the medians (Line2D objects)\n",
    "    for i, median in enumerate(bplot['medians']):\n",
    "        median.set_color(colors[i])\n",
    "    \n",
    "    # Update the color of the whiskers (each box has 2 whiskers)\n",
    "    for i in range(len(positions)):\n",
    "        bplot['whiskers'][2*i].set_color(colors[i])\n",
    "        bplot['whiskers'][2*i+1].set_color(colors[i])\n",
    "    \n",
    "    # If caps are enabled, update their color (Line2D objects)\n",
    "    if 'caps' in bplot:\n",
    "        for i, cap in enumerate(bplot['caps']):\n",
    "            cap.set_color(colors[i//2])  # two caps per box\n",
    "\n",
    "    # Plot the scatter points with jitter (using colors2)\n",
    "    for x, y, c in zip(x_jittered, y_data, colors2):\n",
    "        ax.scatter(x, y, s=100, color=c, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fbb97-5b68-4446-a2d4-0e111eb28635",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pos = np.array([0,0.25,0.5])\n",
    "\n",
    "colors = ['lightseagreen','lightseagreen','lightseagreen']\n",
    "colors2 = ['paleturquoise','paleturquoise','paleturquoise']\n",
    "fig,ax = plt.subplots(figsize=(12,4))\n",
    "y_data = [1000*abs(Min_SBI[i][CMasks[i]][:,-3]-Full_SBI[i][CMasks[i]][:,-3]) for i in range(3)]\n",
    "\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax)\n",
    "\n",
    "g_pos = np.array([1,1.25,1.5])\n",
    "colors = ['darkorange','darkorange','darkorange']\n",
    "colors2 = ['peachpuff','peachpuff','peachpuff']\n",
    "y_data = [1000*abs(Min_LS[i][CMasks[i]][:,-2]-Full_SBI[i][CMasks[i]][:,-3]) for i in range(3)]\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax)\n",
    "\n",
    "g_pos = np.array([2,2.25,2.5])\n",
    "colors = ['darkorange','darkorange','darkorange']\n",
    "colors2 = ['peachpuff','peachpuff','peachpuff']\n",
    "y_data = [1000*abs(Full_LS[i][CMasks[i]][:,-2]-Full_SBI[i][CMasks[i]][:,-3]) for i in range(3)]\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax,True)\n",
    "\n",
    "g_pos = np.array([3,3.25,3.5])\n",
    "colors = ['k','k','k']\n",
    "colors2 = ['gray','gray','gray']\n",
    "y_data = [1000*abs(Full_LS[i][CMasks[i]][:,-2]-Min_LS[i][CMasks[i]][:,-2]) for i in range(3)]\n",
    "\n",
    "BoxPlots(y_data,g_pos,colors,colors2,ax)\n",
    "\n",
    "ax.set_xticks([0.25,1.25,2.25,3.25],['SBI Min','NLLS Min','NLLS Full','NLLS Comp'],fontsize =24)\n",
    "\n",
    "ax.ticklabel_format(axis='y',style='sci',scilimits=(-1,1))\n",
    "ax.tick_params(axis='y', labelsize=24,)\n",
    "ax.yaxis.get_offset_text().set_fontsize(24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224276ba-6155-4636-b3c2-53e3e4f6ee14",
   "metadata": {},
   "source": [
    "## Evaluation using SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec8bf4-0a2a-4526-b353-32b6375a401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_SBI = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[42,45,48],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = posterior.sample((1000,), x=np.append(D[i,j,sl, :],100*(kk+1)),show_progress_bars=False)\n",
    "        return i, j, posterior_samples_1.mean(axis=0)\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [14])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "\n",
    "    Full_SBI.append(NoiseEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5479f4-6e28-405d-96c6-abf074b649fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_SBI = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[42,45,48],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel(i, j):\n",
    "        torch.manual_seed(10)  # If required\n",
    "        posterior_samples_1 = posteriorMin.sample((1000,), x=np.append(D[i,j,sl, IndxArr[kk]],100*(kk+1)),show_progress_bars=False)\n",
    "        return i, j, posterior_samples_1.mean(axis=0)\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(optimize_pixel)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst = np.zeros(list(ArrShape) + [14])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst[i, j] = x\n",
    "\n",
    "    Min_SBI.append(NoiseEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e53ea-e47a-4b1a-adfb-006fd0c13797",
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_LS = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[42,45,48],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel_LS(i, j):\n",
    "        result = sp.optimize.least_squares(residuals_LS_3Indv, guess, args=[BVecs[kk],BVals[kk],D[i,j,sl, :]],\n",
    "                                  bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        return i, j, result.x\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst_LS[i, j] = x\n",
    "\n",
    "    Full_LS.append(NoiseEst_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b28bcc9-ff62-4fae-b65f-5df62a2815ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_LS = []\n",
    "for kk,(D,sl,sma) in enumerate(zip(Datas,[42,45,48],S_masks)):\n",
    "    # Compute the mask where the sum is not zero\n",
    "    mask = np.sum(D[:, :, sl, :], axis=-1) != 0\n",
    "    \n",
    "    # Get the indices where mask is True\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    # Define the function for optimization\n",
    "    def optimize_pixel_LS(i, j):\n",
    "        result = sp.optimize.least_squares(residuals_LS_3Indv_Min, guess, args=[BVecsDev[kk],BValsDev[kk],D[i, j,sl, IndxArr[kk]]],\n",
    "                                  bounds=bounds,verbose=0,xtol=1e-12,gtol=1e-12,ftol=1e-12,jac='3-point')\n",
    "        return i, j, result.x\n",
    "    \n",
    "    # Initialize NoiseEst with the appropriate shape\n",
    "    ArrShape = mask.shape\n",
    "    \n",
    "    # Use joblib to parallelize the optimization tasks\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(optimize_pixel_LS)(i, j) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    NoiseEst_LS = np.zeros(list(ArrShape) + [13])\n",
    "    \n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x in results:\n",
    "        NoiseEst_LS[i, j] = x\n",
    "\n",
    "    Min_LS.append(NoiseEst_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d382d-9019-48e5-9d59-8af2692fce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Full_LS_slice1.npy',Full_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a112b-6041-4c89-b1b8-e6c9d12f71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_LS = np.load('Full_LS_slice1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b471fb-61dd-42eb-ac78-9745d7c4a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_LS = np.load('Min_LS_slice1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef8969d-50f3-49fe-bfb5-ab5502c388cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Par_frac(i,j,Mat):\n",
    "    MD = np.linalg.eigh(vals_to_mat(Mat[i,j]))[0].mean()\n",
    "\n",
    "    FA = FracAni(np.linalg.eigh(vals_to_mat(Mat[i,j]))[0],MD)\n",
    "    return i, j, [FA,MD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bf40ed-b886-4b04-9074-f6facbbde4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "KK = [42,45,48]\n",
    "FA_Full_SBI = []\n",
    "MD_Full_SBI = []\n",
    "for jj in range(3):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(Par_frac)(i, j,Full_SBI[jj][...,4:10]) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x1,x2 in results:\n",
    "        temp1[i, j] = x1\n",
    "        temp2[i, j] = x2\n",
    "\n",
    "    FA_Full_SBI.append(temp1)\n",
    "    MD_Full_SBI.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d55a0-b26c-4f88-ae57-a3796be5a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "KK = [42,45,48]\n",
    "FA_Min_SBI = []\n",
    "MD_Min_SBI = []\n",
    "for jj in range(3):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(Par_frac)(i, j,Min_SBI[jj][...,4:10]) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x1,x2 in results:\n",
    "        temp1[i, j] = x1\n",
    "        temp2[i, j] = x2\n",
    "\n",
    "    FA_Min_SBI.append(temp1)\n",
    "    MD_Min_SBI.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c381327-ecb6-44dd-8818-b1c520829ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "KK = [42,45,48]\n",
    "FA_Full_LS = []\n",
    "MD_Full_LS = []\n",
    "for jj in range(3):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(Par_frac)(i, j,Full_LS[jj][...,4:10]) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x1,x2 in results:\n",
    "        temp1[i, j] = x1\n",
    "        temp2[i, j] = x2\n",
    "\n",
    "    FA_Full_LS.append(temp1)\n",
    "    MD_Full_LS.append(temp2)\n",
    "\n",
    "KK = [42,45,48]\n",
    "FA_Min_LS = []\n",
    "MD_Min_LS = []\n",
    "for jj in range(3):\n",
    "    mask = Outlines[jj][:,:,KK[jj]]\n",
    "    indices = np.argwhere(mask)\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(Par_frac)(i, j,Min_LS[jj][...,4:10]) for i, j in tqdm(indices)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp1 = np.zeros(list(ArrShape))\n",
    "    temp2 = np.zeros(list(ArrShape))\n",
    "    # Assign the optimization results to NoiseEst\n",
    "    for i, j, x1,x2 in results:\n",
    "        temp1[i, j] = x1\n",
    "        temp2[i, j] = x2\n",
    "\n",
    "    FA_Min_LS.append(temp1)\n",
    "    MD_Min_LS.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b15e2-bbce-43cd-9317-a528bfda92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c18ca4-2cc5-4a16-8cd0-39c58fe0b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22964d4-a263-420f-99fb-7cd245c8d8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379c6ab-8f88-4cec-9539-91d7b298b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_cross_correlation(f, g):\n",
    "    \n",
    "    # Subtract the means\n",
    "    f_mean = f - np.mean(f)\n",
    "    g_mean = g - np.mean(g)\n",
    "    \n",
    "    # Compute the numerator and denominators\n",
    "    numerator = np.sum(f_mean * g_mean)\n",
    "    denominator = np.sqrt(np.sum(f_mean ** 2) * np.sum(g_mean ** 2))\n",
    "    \n",
    "    # Handle division by zero\n",
    "    if denominator == 0:\n",
    "        return 0  # Or consider handling it as needed\n",
    "    \n",
    "    ncc = numerator / denominator\n",
    "    return ncc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecb20e3-53d4-4ab3-9e3d-e5f51167aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = 0\n",
    "SBI_comp = []\n",
    "KK = [42,45,48]\n",
    "for i in range(3):\n",
    "    NS1 = np.copy(Min_SBI[i][...,0])\n",
    "    NS2 = np.copy(Full_SBI[i][...,0])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_comp.append(masked_ssim.mean())\n",
    "KK = [42,45,48]\n",
    "LS_comp = []\n",
    "for i in range(3):\n",
    "    NS1 = np.copy(Min_LS[i][...,0])\n",
    "    NS2 = np.copy(Full_LS[i][...,0])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    LS_comp.append(masked_ssim.mean())\n",
    "KK = [42,45,48]\n",
    "SBI_LS_comp = []\n",
    "for i in range(3):\n",
    "    NS1 = np.copy(Full_SBI[i][...,0])\n",
    "    NS2 = np.copy(Full_LS[i][...,0])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_LS_comp.append(masked_ssim.mean())\n",
    "\n",
    "plt.subplots(figsize=(3,2))\n",
    "plt.scatter(1.05*np.ones(3),SBI_comp,s=50,c='paleturquoise', edgecolors='lightseagreen')\n",
    "plt.plot(1.05,np.mean(SBI_comp),'_',ms=20,mew=5,c='lightseagreen')\n",
    "plt.scatter(np.ones(3)*1.1,LS_comp,s=50,c='peachpuff', edgecolors='darkorange')\n",
    "plt.plot(1.1,np.mean(LS_comp),'_',ms=20,mew=5,c='darkorange')\n",
    "plt.scatter(np.ones(3)*1.15,SBI_LS_comp,s=50,c='gray',edgecolors='k')\n",
    "plt.plot(1.15,np.mean(SBI_LS_comp),'_',ms=20,mew=5,c='k')\n",
    "plt.xlim([1,1.2])\n",
    "plt.xticks([1.05,1.1,1.15],['SBI','NLLS','SBI \\n NLLS'],fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylim([0.0,1])\n",
    "plt.grid(axis='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c7170-5232-4038-ac6c-22e70a2b2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Min_SBI[0][...,0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8deaa5e-a669-4fba-b4cd-19820853775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Full_SBI[0][...,0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d871c35-572f-4cc6-a0ab-8f4a0d06a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Full_LS[0][...,0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0c5df-f708-401b-8200-8ec3e89ff17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Min_LS[0][...,0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717c3fd-deb7-444a-bf54-9ee0571af63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = 0\n",
    "SBI_comp = []\n",
    "KK = [42,45,48]\n",
    "for i in range(3):\n",
    "    NS1 = np.copy(Min_SBI[i][...,3])\n",
    "    NS2 = np.copy(Full_SBI[i][...,3])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_comp.append(masked_ssim.mean())\n",
    "KK = [42,45,48]\n",
    "LS_comp = []\n",
    "for i in range(3):\n",
    "    NS1 = np.copy(Min_LS[i][...,3])\n",
    "    NS2 = np.copy(Full_LS[i][...,3])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    LS_comp.append(masked_ssim.mean())\n",
    "KK = [42,45,48]\n",
    "SBI_LS_comp = []\n",
    "for i in range(3):\n",
    "    NS1 = np.copy(Full_SBI[i][...,3])\n",
    "    NS2 = np.copy(Full_LS[i][...,3])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_LS_comp.append(masked_ssim.mean())\n",
    "\n",
    "plt.subplots(figsize=(3,2))\n",
    "plt.scatter(1.05*np.ones(3),SBI_comp,s=50,c='paleturquoise', edgecolors='lightseagreen')\n",
    "plt.plot(1.05,np.mean(SBI_comp),'_',ms=20,mew=5,c='lightseagreen')\n",
    "plt.scatter(np.ones(3)*1.1,LS_comp,s=50,c='peachpuff', edgecolors='darkorange')\n",
    "plt.plot(1.1,np.mean(LS_comp),'_',ms=20,mew=5,c='darkorange')\n",
    "plt.scatter(np.ones(3)*1.15,SBI_LS_comp,s=50,c='gray',edgecolors='k')\n",
    "plt.plot(1.15,np.mean(SBI_LS_comp),'_',ms=20,mew=5,c='k')\n",
    "plt.xlim([1,1.2])\n",
    "plt.xticks([1.05,1.1,1.15],['SBI','NLLS','SBI \\n NLLS'],fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylim([0.0,1])\n",
    "plt.grid(axis='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071bc580-a3ef-4502-bb1e-062ff6c97ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = 0\n",
    "SBI_comp = []\n",
    "KK = [42,45,48]\n",
    "for i in range(3):\n",
    "    NS1 = np.copy(MD_Min_SBI[i])\n",
    "    NS2 = np.copy(MD_Full_SBI[i])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_comp.append(masked_ssim.mean())\n",
    "KK = [42,45,48]\n",
    "LS_comp = []\n",
    "for i in range(3):\n",
    "    NS1 = np.copy(MD_Min_LS[i])\n",
    "    NS2 = np.copy(MD_Full_LS[i])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    LS_comp.append(masked_ssim.mean())\n",
    "KK = [42,45,48]\n",
    "SBI_LS_comp = []\n",
    "for i in range(3):\n",
    "    NS1 = np.copy(MD_Full_SBI[i])\n",
    "    NS2 = np.copy(MD_Full_LS[i])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_LS_comp.append(masked_ssim.mean())\n",
    "\n",
    "plt.subplots(figsize=(3,2))\n",
    "plt.scatter(1.05*np.ones(3),SBI_comp,s=50,c='paleturquoise', edgecolors='lightseagreen')\n",
    "plt.plot(1.05,np.mean(SBI_comp),'_',ms=20,mew=5,c='lightseagreen')\n",
    "plt.scatter(np.ones(3)*1.1,LS_comp,s=50,c='peachpuff', edgecolors='darkorange')\n",
    "plt.plot(1.1,np.mean(LS_comp),'_',ms=20,mew=5,c='darkorange')\n",
    "plt.scatter(np.ones(3)*1.15,SBI_LS_comp,s=50,c='gray',edgecolors='k')\n",
    "plt.plot(1.15,np.mean(SBI_LS_comp),'_',ms=20,mew=5,c='k')\n",
    "plt.xlim([1,1.2])\n",
    "plt.xticks([1.05,1.1,1.15],['SBI','NLLS','SBI \\n NLLS'],fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylim([0.0,1])\n",
    "plt.grid(axis='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2e1b9-a025-4b2a-9fc2-19794d3fca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = 0\n",
    "SBI_comp = []\n",
    "KK = [42,45,48]\n",
    "for i in range(3):\n",
    "    NS1 = np.copy(Min_SBI[i][...,-4])\n",
    "    NS2 = np.copy(Full_SBI[i][...,-4])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_comp.append(masked_ssim.mean())\n",
    "KK = [42,45,48]\n",
    "LS_comp = []\n",
    "for i in range(3):\n",
    "    NS1 = np.copy(Min_LS[i][...,-3])\n",
    "    NS2 = np.copy(Full_LS[i][...,-3])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    LS_comp.append(masked_ssim.mean())\n",
    "KK = [42,45,48]\n",
    "SBI_LS_comp = []\n",
    "for i in range(3):\n",
    "    NS1 = np.copy(Full_SBI[i][...,-4])\n",
    "    NS2 = np.copy(Full_LS[i][...,-3])\n",
    "\n",
    "    core,ssim_map = ssim(NS1,NS2, data_range=max([NS1.max(),NS2.max()])-min([NS1.min(),NS2.min()]),full=True,win_size=15)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    SBI_LS_comp.append(masked_ssim.mean())\n",
    "\n",
    "plt.subplots(figsize=(3,2))\n",
    "plt.scatter(1.05*np.ones(3),SBI_comp,s=50,c='paleturquoise', edgecolors='lightseagreen')\n",
    "plt.plot(1.05,np.mean(SBI_comp),'_',ms=20,mew=5,c='lightseagreen')\n",
    "plt.scatter(np.ones(3)*1.1,LS_comp,s=50,c='peachpuff', edgecolors='darkorange')\n",
    "plt.plot(1.1,np.mean(LS_comp),'_',ms=20,mew=5,c='darkorange')\n",
    "plt.scatter(np.ones(3)*1.15,SBI_LS_comp,s=50,c='gray',edgecolors='k')\n",
    "plt.plot(1.15,np.mean(SBI_LS_comp),'_',ms=20,mew=5,c='k')\n",
    "plt.xlim([1,1.2])\n",
    "plt.xticks([1.05,1.1,1.15],['SBI','NLLS','SBI \\n NLLS'],fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylim([0.0,1])\n",
    "plt.grid(axis='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ead1c-dbec-4582-94af-a8c25c0a4b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBI_comp = []\n",
    "KK = [42,45,48]\n",
    "for i in range(3):\n",
    "    SBI_comp.append(ssim(FA_Full_SBI[i],FA_Min_SBI[i], data_range=1))\n",
    "KK = [42,45,48]\n",
    "LS_comp = []\n",
    "for i in range(3):\n",
    "    LS_comp.append(ssim(FA_Full_LS[i],FA_Min_LS[i], data_range=1))\n",
    "KK = [42,45,48]\n",
    "SBI_LS_comp = []\n",
    "for i in range(3):\n",
    "    SBI_LS_comp.append(ssim(FA_Full_LS[i],FA_Full_SBI[i], data_range=1))\n",
    "\n",
    "plt.subplots(figsize=(3,4))\n",
    "plt.scatter(1.05*np.ones(3),SBI_comp,s=50,c='paleturquoise', edgecolors='lightseagreen')\n",
    "plt.plot(1.05,np.mean(SBI_comp),'_',ms=20,mew=5,c='lightseagreen')\n",
    "plt.scatter(np.ones(3)*1.1,LS_comp,s=50,c='peachpuff', edgecolors='darkorange')\n",
    "plt.plot(1.1,np.mean(LS_comp),'_',ms=20,mew=5,c='darkorange')\n",
    "plt.scatter(np.ones(3)*1.15,SBI_LS_comp,s=50,c='gray',edgecolors='k')\n",
    "plt.plot(1.15,np.mean(SBI_LS_comp),'_',ms=20,mew=5,c='k')\n",
    "plt.xlim([1,1.2])\n",
    "plt.xticks([1.05,1.1,1.15],['SBI','NLLS','SBI \\n NLLS'],fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylim([0.0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1163d1-a6db-45fc-b0d0-f5b1c9b5c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb53fc86-fef9-4c2c-abf0-7377be231671",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(FA_Min_LS[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a098841-4410-470e-9cb5-b2cba4508645",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(~Outlines[i][:,:,KK[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9fb93a-59ef-432b-81ba-e372e7f4fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "NS2[~Outlines[i][:,:,KK[i]]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be3ffca-d017-472b-9a6b-99a13750bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    score,ssim_map = ssim(FA_Full_LS[i],FA_Min_LS[i], data_range=1,full=True,win_size=15)\n",
    "    mask = np.zeros_like(ssim_map, dtype=bool)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    print(f\"Overall SSIM: {score}\")\n",
    "    print(f\"Masked SSIM: {masked_ssim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3cadd7-8d57-4389-83fa-7a04c13045d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    score,ssim_map = ssim(Min_SBI[i][...,0],Full_SBI[i][...,0], data_range=5e-5,full=True,win_size=15)\n",
    "    mask = np.zeros_like(ssim_map, dtype=bool)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    print(f\"Overall SSIM: {score}\")\n",
    "    print(f\"Masked SSIM: {masked_ssim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231c179-3044-47e8-b2b1-5af9105c5fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    score,ssim_map = ssim(Min_LS[i][...,-3],Full_LS[i][...,-3], data_range=1,full=True,win_size=15)\n",
    "    mask = np.zeros_like(ssim_map, dtype=bool)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    print(f\"Overall SSIM: {score}\")\n",
    "    print(f\"Masked SSIM: {masked_ssim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dce275-86df-4780-bcc9-9855c6216aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    score,ssim_map = ssim(Min_SBI[i][...,4],Full_SBI[i][...,4], data_range=5e-5,full=True,win_size=15)\n",
    "    mask = np.zeros_like(ssim_map, dtype=bool)\n",
    "    masked_ssim = ssim_map[Outlines[i][:,:,KK[i]]].mean()\n",
    "    print(f\"Overall SSIM: {score}\")\n",
    "    print(f\"Masked SSIM: {masked_ssim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0ce3f-2a8b-4d79-b93e-397f3744da87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ecc035-9ed4-470b-aefe-34b987aeb923",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Full_SBI[i][...,1]-Min_SBI[i][...,1])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88161ee9-75af-4648-8969-38f39d198a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Min_SBI[i][...,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df478f3d-a633-4768-a76d-73a36d124c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    NS1 = np.copy(Min_SBI[i])[Outlines[i][:,:,KK[i]]]\n",
    "    NS2 = np.copy(Full_SBI[i])[Outlines[i][:,:,KK[i]]]\n",
    "    print(np.corrcoef(NS1[:,4],NS2[:,4])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4492fc9-0a5f-48d4-8ee8-129bdd80a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    NS1 = np.copy(Min_SBI[i])[Outlines[i][:,:,KK[i]]]\n",
    "    NS2 = np.copy(Full_SBI[i])[Outlines[i][:,:,KK[i]]]\n",
    "    print(np.corrcoef(NS1[:,4],NS2[:,4])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c9846-ef93-4246-be45-40cae6404100",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    NS1 = np.copy(Min_LS[i])[Outlines[i][:,:,KK[i]]]\n",
    "    NS2 = np.copy(Full_LS[i])[Outlines[i][:,:,KK[i]]]\n",
    "    print(np.corrcoef(NS1[:,-4],NS2[:,-4])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f1aa18-54ac-41ac-be08-96fb04fc8e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "NS1[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cbcc99-ceca-458b-847e-ebcbc6810ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010ec03-0284-4cf4-954d-9e4c8d7c006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(NS1[:,0],NS2[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb230001-f1fe-46a1-93d9-5998eefd222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NS1 = np.copy(Min_SBI[i])\n",
    "NS1[~Outlines[i][:,:,KK[i]]] = 0\n",
    "\n",
    "NS2 = np.copy(Full_SBI[i])\n",
    "NS2[~Outlines[i][:,:,KK[i]]] = 0\n",
    "#SBI_comp.append(ssim(NS1[...,-4], NS2[...,-4], data_range=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31591020-2ada-417d-a37b-91e257fab8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9c825-c9ea-4ed9-a7d7-82b534ab63fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_map[~Outlines[i][:,:,KK[i]]] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80ad9d9-2597-4c37-8175-4209402d871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = np.copy(Full_SBI[i][...,-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3890b5-e079-47db-b746-191a53dc80b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1[~Outlines[i][:,:,KK[i]]] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bec4bf-2c7e-4204-bad8-c97f1b6b222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ssim_map[~Outlines[i][:,:,KK[i]]] = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf98cd7c-7952-4076-aa8f-ae72a6d63cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = abs(Min_SBI[i][...,-4]-Full_SBI[i][...,-4])[abs(Min_SBI[i][...,-4]-Full_SBI[i][...,-4])>0]\n",
    "plt.boxplot(X2,showfliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb403f-d317-4f6b-9190-a1e62cf6b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = abs(Min_LS[i][...,-3]-Full_LS[i][...,-3])[abs(Min_LS[i][...,-3]-Full_LS[i][...,-3])>0]\n",
    "plt.boxplot(X,showfliers=False)\n",
    "plt.boxplot(X2,showfliers=False,positions=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb1cacd-5c93-4694-852a-3a3d94ae0cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a16f2fa-2cb9-40ba-a9d3-e1afe9e85913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ea37c-dc23-4363-9a8f-a2d35bc01809",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a807e70-62cd-49e2-939f-68a8cc81371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Min_SBI[i][...,-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753bd06f-44cf-4588-90fe-1a7214e5bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(ssim_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce27268-9e7d-4189-aa47-983c4193c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ssim_map)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d47712-f59c-45e1-a6cb-7adf22c2915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FA_Full_SBI[i][Outlines[i][:,:,KK[i]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc575f1-196e-47d3-8a65-ab8ca0b763a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FA_Full_SBI[i][Outlines[i][:,:,KK[i]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f74b64-288e-4b67-936a-d5bc652f724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "score, ssim_map = ssim(FA_Full_LS[i],FA_Min_LS[i],data_range=1,full = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d009d-53b8-4c04-830a-12be55d4c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_map[Outlines[i][:,:,KK[i]]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd24b3-5dbd-4fdd-929a-627b0c7f41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(FA_Min_SBI[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38399220-a073-4c71-89e8-3a2888fdc4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(FA_Full_SBI[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f987c75-1ad6-47f5-9b0f-63a2ac76707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "ssim(FA_Full_LS[i][Outlines[i][:,:,KK[i]]],FA_Min_LS[i][Outlines[i][:,:,KK[i]]],data_range=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f98c83-358c-43cc-a8db-2877b3db890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "psnr(FA_Full_SBI[i],FA_Min_SBI[i],data_range=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998bdef-6c15-42cf-b903-edbe5848bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(FA_Full_LS[i])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9fac9-089f-45d9-bb11-3b8160bb9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Full_SBI[0][...,-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832aab2a-e215-4601-9ce7-cda8575a1ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Min_SBI[0][...,-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7560382-f008-4f89-a4ce-36c7de475515",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(FA_Min_LS[i],vmin=0,vmax=1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d46000-dc23-48c2-bf8e-86d3f4f0a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "FA_Min_LS[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70498dbd-0218-444c-8a3a-fc4ccc7130b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
